{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /raid/jupyter-rick.109101013.md0-1ced0/.conda/envs/seg/lib/python3.9/site-packages (1.9.0+cu111)\n",
      "Requirement already satisfied: torchvision in /raid/jupyter-rick.109101013.md0-1ced0/.conda/envs/seg/lib/python3.9/site-packages (0.10.0+cu111)\n",
      "Requirement already satisfied: torchaudio in /raid/jupyter-rick.109101013.md0-1ced0/.conda/envs/seg/lib/python3.9/site-packages (0.9.0)\n",
      "Requirement already satisfied: typing-extensions in /raid/jupyter-rick.109101013.md0-1ced0/.conda/envs/seg/lib/python3.9/site-packages (from torch) (4.11.0)\n",
      "Requirement already satisfied: numpy in /raid/jupyter-rick.109101013.md0-1ced0/.conda/envs/seg/lib/python3.9/site-packages (from torchvision) (1.26.4)\n",
      "Requirement already satisfied: pillow>=5.3.0 in /raid/jupyter-rick.109101013.md0-1ced0/.conda/envs/seg/lib/python3.9/site-packages (from torchvision) (10.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: torch in /raid/jupyter-rick.109101013.md0-1ced0/.conda/envs/seg/lib/python3.9/site-packages (from -r requirements.txt (line 1)) (1.9.0+cu111)\n",
      "Requirement already satisfied: torchvision==0.10.0 in /raid/jupyter-rick.109101013.md0-1ced0/.conda/envs/seg/lib/python3.9/site-packages (from -r requirements.txt (line 2)) (0.10.0+cu111)\n",
      "Requirement already satisfied: numpy in /raid/jupyter-rick.109101013.md0-1ced0/.conda/envs/seg/lib/python3.9/site-packages (from -r requirements.txt (line 3)) (1.26.4)\n",
      "Requirement already satisfied: matplotlib in /raid/jupyter-rick.109101013.md0-1ced0/.conda/envs/seg/lib/python3.9/site-packages (from -r requirements.txt (line 4)) (3.8.4)\n",
      "Requirement already satisfied: pillow==10.3.0 in /raid/jupyter-rick.109101013.md0-1ced0/.conda/envs/seg/lib/python3.9/site-packages (from -r requirements.txt (line 5)) (10.3.0)\n",
      "Requirement already satisfied: scikit-learn in /raid/jupyter-rick.109101013.md0-1ced0/.conda/envs/seg/lib/python3.9/site-packages (from -r requirements.txt (line 6)) (1.4.2)\n",
      "Requirement already satisfied: pytorch_ssim==0.1 in /raid/jupyter-rick.109101013.md0-1ced0/.conda/envs/seg/lib/python3.9/site-packages (from -r requirements.txt (line 7)) (0.1)\n",
      "Requirement already satisfied: albumentations==1.4.4 in /raid/jupyter-rick.109101013.md0-1ced0/.conda/envs/seg/lib/python3.9/site-packages (from -r requirements.txt (line 8)) (1.4.4)\n",
      "Requirement already satisfied: segmentation_models_pytorch==0.3.3 in /raid/jupyter-rick.109101013.md0-1ced0/.conda/envs/seg/lib/python3.9/site-packages (from -r requirements.txt (line 9)) (0.3.3)\n",
      "Requirement already satisfied: opencv-python==4.9.0.80 in /raid/jupyter-rick.109101013.md0-1ced0/.conda/envs/seg/lib/python3.9/site-packages (from -r requirements.txt (line 10)) (4.9.0.80)\n",
      "Requirement already satisfied: torchinfo==1.8.0 in /raid/jupyter-rick.109101013.md0-1ced0/.conda/envs/seg/lib/python3.9/site-packages (from -r requirements.txt (line 11)) (1.8.0)\n",
      "Requirement already satisfied: typing-extensions in /raid/jupyter-rick.109101013.md0-1ced0/.conda/envs/seg/lib/python3.9/site-packages (from torch->-r requirements.txt (line 1)) (4.11.0)\n",
      "Requirement already satisfied: scipy>=1.10.0 in /raid/jupyter-rick.109101013.md0-1ced0/.conda/envs/seg/lib/python3.9/site-packages (from albumentations==1.4.4->-r requirements.txt (line 8)) (1.13.0)\n",
      "Requirement already satisfied: scikit-image>=0.21.0 in /raid/jupyter-rick.109101013.md0-1ced0/.conda/envs/seg/lib/python3.9/site-packages (from albumentations==1.4.4->-r requirements.txt (line 8)) (0.22.0)\n",
      "Requirement already satisfied: PyYAML in /raid/jupyter-rick.109101013.md0-1ced0/.conda/envs/seg/lib/python3.9/site-packages (from albumentations==1.4.4->-r requirements.txt (line 8)) (6.0.1)\n",
      "Requirement already satisfied: pydantic>=2.6.4 in /raid/jupyter-rick.109101013.md0-1ced0/.conda/envs/seg/lib/python3.9/site-packages (from albumentations==1.4.4->-r requirements.txt (line 8)) (2.7.0)\n",
      "Requirement already satisfied: opencv-python-headless>=4.9.0 in /raid/jupyter-rick.109101013.md0-1ced0/.conda/envs/seg/lib/python3.9/site-packages (from albumentations==1.4.4->-r requirements.txt (line 8)) (4.9.0.80)\n",
      "Requirement already satisfied: pretrainedmodels==0.7.4 in /raid/jupyter-rick.109101013.md0-1ced0/.conda/envs/seg/lib/python3.9/site-packages (from segmentation_models_pytorch==0.3.3->-r requirements.txt (line 9)) (0.7.4)\n",
      "Requirement already satisfied: efficientnet-pytorch==0.7.1 in /raid/jupyter-rick.109101013.md0-1ced0/.conda/envs/seg/lib/python3.9/site-packages (from segmentation_models_pytorch==0.3.3->-r requirements.txt (line 9)) (0.7.1)\n",
      "Requirement already satisfied: timm==0.9.7 in /raid/jupyter-rick.109101013.md0-1ced0/.conda/envs/seg/lib/python3.9/site-packages (from segmentation_models_pytorch==0.3.3->-r requirements.txt (line 9)) (0.9.7)\n",
      "Requirement already satisfied: tqdm in /raid/jupyter-rick.109101013.md0-1ced0/.conda/envs/seg/lib/python3.9/site-packages (from segmentation_models_pytorch==0.3.3->-r requirements.txt (line 9)) (4.66.2)\n",
      "Requirement already satisfied: six in /raid/jupyter-rick.109101013.md0-1ced0/.conda/envs/seg/lib/python3.9/site-packages (from segmentation_models_pytorch==0.3.3->-r requirements.txt (line 9)) (1.16.0)\n",
      "Requirement already satisfied: munch in /raid/jupyter-rick.109101013.md0-1ced0/.conda/envs/seg/lib/python3.9/site-packages (from pretrainedmodels==0.7.4->segmentation_models_pytorch==0.3.3->-r requirements.txt (line 9)) (4.0.0)\n",
      "Requirement already satisfied: huggingface-hub in /raid/jupyter-rick.109101013.md0-1ced0/.conda/envs/seg/lib/python3.9/site-packages (from timm==0.9.7->segmentation_models_pytorch==0.3.3->-r requirements.txt (line 9)) (0.22.2)\n",
      "Requirement already satisfied: safetensors in /raid/jupyter-rick.109101013.md0-1ced0/.conda/envs/seg/lib/python3.9/site-packages (from timm==0.9.7->segmentation_models_pytorch==0.3.3->-r requirements.txt (line 9)) (0.4.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /raid/jupyter-rick.109101013.md0-1ced0/.conda/envs/seg/lib/python3.9/site-packages (from matplotlib->-r requirements.txt (line 4)) (1.2.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /raid/jupyter-rick.109101013.md0-1ced0/.conda/envs/seg/lib/python3.9/site-packages (from matplotlib->-r requirements.txt (line 4)) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /raid/jupyter-rick.109101013.md0-1ced0/.conda/envs/seg/lib/python3.9/site-packages (from matplotlib->-r requirements.txt (line 4)) (4.51.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /raid/jupyter-rick.109101013.md0-1ced0/.conda/envs/seg/lib/python3.9/site-packages (from matplotlib->-r requirements.txt (line 4)) (1.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /raid/jupyter-rick.109101013.md0-1ced0/.conda/envs/seg/lib/python3.9/site-packages (from matplotlib->-r requirements.txt (line 4)) (24.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /raid/jupyter-rick.109101013.md0-1ced0/.conda/envs/seg/lib/python3.9/site-packages (from matplotlib->-r requirements.txt (line 4)) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /raid/jupyter-rick.109101013.md0-1ced0/.conda/envs/seg/lib/python3.9/site-packages (from matplotlib->-r requirements.txt (line 4)) (2.8.2)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in /raid/jupyter-rick.109101013.md0-1ced0/.conda/envs/seg/lib/python3.9/site-packages (from matplotlib->-r requirements.txt (line 4)) (6.4.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /raid/jupyter-rick.109101013.md0-1ced0/.conda/envs/seg/lib/python3.9/site-packages (from scikit-learn->-r requirements.txt (line 6)) (1.4.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /raid/jupyter-rick.109101013.md0-1ced0/.conda/envs/seg/lib/python3.9/site-packages (from scikit-learn->-r requirements.txt (line 6)) (3.4.0)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /raid/jupyter-rick.109101013.md0-1ced0/.conda/envs/seg/lib/python3.9/site-packages (from importlib-resources>=3.2.0->matplotlib->-r requirements.txt (line 4)) (3.11.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /raid/jupyter-rick.109101013.md0-1ced0/.conda/envs/seg/lib/python3.9/site-packages (from pydantic>=2.6.4->albumentations==1.4.4->-r requirements.txt (line 8)) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.18.1 in /raid/jupyter-rick.109101013.md0-1ced0/.conda/envs/seg/lib/python3.9/site-packages (from pydantic>=2.6.4->albumentations==1.4.4->-r requirements.txt (line 8)) (2.18.1)\n",
      "Requirement already satisfied: networkx>=2.8 in /raid/jupyter-rick.109101013.md0-1ced0/.conda/envs/seg/lib/python3.9/site-packages (from scikit-image>=0.21.0->albumentations==1.4.4->-r requirements.txt (line 8)) (3.2.1)\n",
      "Requirement already satisfied: imageio>=2.27 in /raid/jupyter-rick.109101013.md0-1ced0/.conda/envs/seg/lib/python3.9/site-packages (from scikit-image>=0.21.0->albumentations==1.4.4->-r requirements.txt (line 8)) (2.34.0)\n",
      "Requirement already satisfied: tifffile>=2022.8.12 in /raid/jupyter-rick.109101013.md0-1ced0/.conda/envs/seg/lib/python3.9/site-packages (from scikit-image>=0.21.0->albumentations==1.4.4->-r requirements.txt (line 8)) (2024.2.12)\n",
      "Requirement already satisfied: lazy_loader>=0.3 in /raid/jupyter-rick.109101013.md0-1ced0/.conda/envs/seg/lib/python3.9/site-packages (from scikit-image>=0.21.0->albumentations==1.4.4->-r requirements.txt (line 8)) (0.4)\n",
      "Requirement already satisfied: filelock in /raid/jupyter-rick.109101013.md0-1ced0/.conda/envs/seg/lib/python3.9/site-packages (from huggingface-hub->timm==0.9.7->segmentation_models_pytorch==0.3.3->-r requirements.txt (line 9)) (3.13.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /raid/jupyter-rick.109101013.md0-1ced0/.conda/envs/seg/lib/python3.9/site-packages (from huggingface-hub->timm==0.9.7->segmentation_models_pytorch==0.3.3->-r requirements.txt (line 9)) (2024.3.1)\n",
      "Requirement already satisfied: requests in /raid/jupyter-rick.109101013.md0-1ced0/.conda/envs/seg/lib/python3.9/site-packages (from huggingface-hub->timm==0.9.7->segmentation_models_pytorch==0.3.3->-r requirements.txt (line 9)) (2.31.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /raid/jupyter-rick.109101013.md0-1ced0/.conda/envs/seg/lib/python3.9/site-packages (from requests->huggingface-hub->timm==0.9.7->segmentation_models_pytorch==0.3.3->-r requirements.txt (line 9)) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /raid/jupyter-rick.109101013.md0-1ced0/.conda/envs/seg/lib/python3.9/site-packages (from requests->huggingface-hub->timm==0.9.7->segmentation_models_pytorch==0.3.3->-r requirements.txt (line 9)) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /raid/jupyter-rick.109101013.md0-1ced0/.conda/envs/seg/lib/python3.9/site-packages (from requests->huggingface-hub->timm==0.9.7->segmentation_models_pytorch==0.3.3->-r requirements.txt (line 9)) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /raid/jupyter-rick.109101013.md0-1ced0/.conda/envs/seg/lib/python3.9/site-packages (from requests->huggingface-hub->timm==0.9.7->segmentation_models_pytorch==0.3.3->-r requirements.txt (line 9)) (2024.2.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/raid/jupyter-rick.109101013.md0-1ced0/.conda/envs/seg/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/Po-Hsun-Su/pytorch-ssim.git\n",
      "  Cloning https://github.com/Po-Hsun-Su/pytorch-ssim.git to /tmp/pip-req-build-vxqctjn2\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/Po-Hsun-Su/pytorch-ssim.git /tmp/pip-req-build-vxqctjn2\n",
      "  Resolved https://github.com/Po-Hsun-Su/pytorch-ssim.git to commit 3add4532d3f633316cba235da1c69e90f0dfb952\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install torch torchvision torchaudio\n",
    "%pip install -r requirements.txt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, TensorDataset, DataLoader, random_split\n",
    "from torchvision import transforms\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import argparse\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "import albumentations as albu\n",
    "import segmentation_models_pytorch as smp\n",
    "import segmentation_models_pytorch.utils as utils\n",
    "import os\n",
    "import cv2\n",
    "import math\n",
    "%pip install git+https://github.com/Po-Hsun-Su/pytorch-ssim.git\n",
    "import pytorch_ssim\n",
    "from torchinfo import summary\n",
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context # for downloading pretrained encoder weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Settings for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 2024\n",
    "GPU_ID = 6 # Select GPU to train and test on\n",
    "EPOCHS = 100\n",
    "LR = 1e-3\n",
    "DECAY = 1e-3\n",
    "OPTIM = 'adam'\n",
    "L2REG = 1e-5\n",
    "ENCODER = 'se_resnet152'\n",
    "ETA = 1\n",
    "PROB = 0.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device set to cuda:6.\n"
     ]
    }
   ],
   "source": [
    "# Set device to GPU if available, else CPU\n",
    "DEVICE = torch.device(f'cuda:{GPU_ID}' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Device set to {DEVICE}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random seed set to 2024\n"
     ]
    }
   ],
   "source": [
    "# Sets random seed for reproducibility (Default = 2024)\n",
    "def set_random_seed(seed=2024):\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.use_deterministic_algorithms(True)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "    print(f\"Random seed set to {seed}\")\n",
    "\n",
    "set_random_seed(SEED)\n",
    "\n",
    "def to_tensor(x, **kwargs):\n",
    "    return x.transpose(2, 0, 1).astype('float32')\n",
    "\n",
    "# Calculate the learning rate based on a linear warmup and a cosine decay.\n",
    "def get_lr(epoch_num, warmup_epochs, total_epochs, init_lr, min_lr):\n",
    "    if epoch_num < warmup_epochs:\n",
    "        # Linear warmup\n",
    "        lr = min_lr + (init_lr - min_lr) * epoch_num / warmup_epochs\n",
    "    else:\n",
    "        # Cosine decay\n",
    "        decay_progress = (epoch_num - warmup_epochs) / (total_epochs - warmup_epochs)\n",
    "        lr = min_lr + (init_lr - min_lr) * (1 + math.cos(math.pi * decay_progress)) / 2\n",
    "    return lr\n",
    "\n",
    "# Log the results of the training session\n",
    "def log_results(filename, config, best_score, best_epoch, final_val_score):\n",
    "    with open(filename, 'a') as file:\n",
    "        file.write(f\"Model Configuration and Training Settings:\\n\")\n",
    "        for key, value in config.items():\n",
    "            file.write(f\"{key}: {value}\\n\")\n",
    "        file.write(f\"Best Validation F-Score: {best_score} (Epoch: {best_epoch + 1})\\n\")\n",
    "        file.write(f\"Final Validation F-Score: {final_val_score}\\n\")\n",
    "        file.write(\"--------------------------------------------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset, augmentation, and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom dataset for images and corresponding masks\n",
    "class NavImgDataset(Dataset):\n",
    "    def __init__(self, img_paths, label_paths, preprocessing=None, augmentation=None):\n",
    "        self.img_paths = img_paths\n",
    "        self.label_paths = label_paths\n",
    "        self.preprocessing = preprocessing\n",
    "        self.augmentation = augmentation\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_paths)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img = cv2.imread(self.img_paths[index])\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        label = cv2.imread(self.label_paths[index], cv2.IMREAD_GRAYSCALE)\n",
    "        label = np.expand_dims(label, axis=-1)\n",
    "        label = label / 255.0\n",
    "        if self.augmentation:\n",
    "            sample = self.augmentation(image=img, mask=label)\n",
    "            img, label = sample['image'], sample['mask']\n",
    "        if self.preprocessing:\n",
    "            sample = self.preprocessing(image=img, mask=label)\n",
    "            img, label = sample['image'], sample['mask']\n",
    "        return img, label\n",
    "\n",
    "# Data augmentation (Flip, blur and noise)\n",
    "def get_training_augmentation():\n",
    "    train_transform = [\n",
    "        # Flips\n",
    "        albu.OneOf(\n",
    "            [\n",
    "                albu.HorizontalFlip(p=1),\n",
    "                albu.VerticalFlip(p=1),\n",
    "            ],\n",
    "            p=0.5,\n",
    "        ),\n",
    "        # Blur and noise\n",
    "        albu.OneOf(\n",
    "            [\n",
    "                albu.Defocus(p=1),\n",
    "                albu.GaussNoise(p=1),\n",
    "                albu.MedianBlur(blur_limit=3, p=1),\n",
    "                albu.MotionBlur(blur_limit=3, p=1),\n",
    "                albu.ZoomBlur(p=1)\n",
    "            ],\n",
    "            p=PROB\n",
    "        ),\n",
    "        # Padding to ensure each dimension is divisible by 32\n",
    "        albu.PadIfNeeded(min_height=256, min_width=448, always_apply=True, border_mode=0),\n",
    "        # Contrast enhancement\n",
    "        albu.CLAHE(clip_limit=2.0, tile_grid_size=(8, 8), always_apply=True)\n",
    "    ]\n",
    "    return albu.Compose(train_transform)\n",
    "\n",
    "# Same preprocessing for validation / test set\n",
    "def get_validation_augmentation():\n",
    "    test_transform = [\n",
    "        albu.PadIfNeeded(min_height=256, min_width=448, always_apply=True, border_mode=0),\n",
    "        albu.CLAHE(clip_limit=2.0, tile_grid_size=(8, 8), always_apply=True)\n",
    "    ]\n",
    "    return albu.Compose(test_transform)\n",
    "\n",
    "def get_preprocessing(preprocessing_fn):    \n",
    "    _transform = [\n",
    "        albu.Lambda(image=preprocessing_fn),\n",
    "        albu.Lambda(image=to_tensor, mask=to_tensor),\n",
    "    ]\n",
    "    return albu.Compose(_transform)\n",
    "\n",
    "def load_split_dataset(img_dir, label_dir,  preprocessing_fn, valid_size=.2):\n",
    "    img_files = sorted([os.path.join(img_dir, f) for f in os.listdir(img_dir) if f.endswith('.jpg')])\n",
    "    label_files = sorted([os.path.join(label_dir, f) for f in os.listdir(label_dir) if f.endswith('.png')])\n",
    "\n",
    "    # Split into training and validation sets (80-20 split)\n",
    "    train_imgs, valid_imgs, train_labels, valid_labels = train_test_split(\n",
    "        img_files, label_files, test_size=valid_size, random_state=2024)\n",
    "\n",
    "    # Create dataset objects\n",
    "    train_dataset = NavImgDataset(\n",
    "        train_imgs, \n",
    "        train_labels, \n",
    "        preprocessing=get_preprocessing(preprocessing_fn), \n",
    "        augmentation=get_training_augmentation()\n",
    "    )\n",
    "    valid_dataset = NavImgDataset(\n",
    "        valid_imgs, \n",
    "        valid_labels, \n",
    "        preprocessing=get_preprocessing(preprocessing_fn),\n",
    "        augmentation=get_validation_augmentation()\n",
    "    )\n",
    "\n",
    "    return train_dataset, valid_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cost function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A combined loss function\n",
    "class CombinedLoss(nn.Module):\n",
    "    def __init__(self, _eta=ETA):\n",
    "        super(CombinedLoss, self).__init__()\n",
    "        self.mcc_loss = smp.losses.MCCLoss()\n",
    "        self.focal_loss = smp.losses.FocalLoss(mode='binary', alpha=0.25, gamma=2.0)\n",
    "        self.ssim_loss = pytorch_ssim.SSIM(window_size=11,size_average=True)\n",
    "        self.eta = _eta\n",
    "        self.__name__ = 'combined_loss'\n",
    "\n",
    "    def forward(self, outputs, targets):\n",
    "        mcc_loss = self.mcc_loss(outputs, targets)\n",
    "        focal_loss = self.focal_loss(outputs, targets)\n",
    "        ssim_loss = 1 - self.ssim_loss(outputs, targets)\n",
    "        combined_loss = mcc_loss + self.eta * focal_loss + ssim_loss\n",
    "        return combined_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/raid/jupyter-rick.109101013.md0-1ced0/.conda/envs/seg/lib/python3.9/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "====================================================================================================\n",
       "Layer (type:depth-idx)                             Output Shape              Param #\n",
       "====================================================================================================\n",
       "Unet                                               [1, 1, 256, 448]          --\n",
       "├─SENetEncoder: 1-1                                [1, 3, 256, 448]          --\n",
       "│    └─Sequential: 2-1                             --                        --\n",
       "│    │    └─Conv2d: 3-1                            [1, 64, 128, 224]         9,408\n",
       "│    │    └─BatchNorm2d: 3-2                       [1, 64, 128, 224]         128\n",
       "│    │    └─ReLU: 3-3                              [1, 64, 128, 224]         --\n",
       "│    │    └─MaxPool2d: 3-4                         [1, 64, 64, 112]          --\n",
       "│    └─Sequential: 2-2                             [1, 256, 64, 112]         --\n",
       "│    │    └─SEResNetBottleneck: 3-5                [1, 256, 64, 112]         83,472\n",
       "│    │    └─SEResNetBottleneck: 3-6                [1, 256, 64, 112]         78,864\n",
       "│    │    └─SEResNetBottleneck: 3-7                [1, 256, 64, 112]         78,864\n",
       "│    └─Sequential: 2-3                             [1, 512, 32, 56]          --\n",
       "│    │    └─SEResNetBottleneck: 3-8                [1, 512, 32, 56]          412,704\n",
       "│    │    └─SEResNetBottleneck: 3-9                [1, 512, 32, 56]          313,376\n",
       "│    │    └─SEResNetBottleneck: 3-10               [1, 512, 32, 56]          313,376\n",
       "│    │    └─SEResNetBottleneck: 3-11               [1, 512, 32, 56]          313,376\n",
       "│    │    └─SEResNetBottleneck: 3-12               [1, 512, 32, 56]          313,376\n",
       "│    │    └─SEResNetBottleneck: 3-13               [1, 512, 32, 56]          313,376\n",
       "│    │    └─SEResNetBottleneck: 3-14               [1, 512, 32, 56]          313,376\n",
       "│    │    └─SEResNetBottleneck: 3-15               [1, 512, 32, 56]          313,376\n",
       "│    └─Sequential: 2-4                             [1, 1024, 16, 28]         --\n",
       "│    │    └─SEResNetBottleneck: 3-16               [1, 1024, 16, 28]         1,644,608\n",
       "│    │    └─SEResNetBottleneck: 3-17               [1, 1024, 16, 28]         1,249,344\n",
       "│    │    └─SEResNetBottleneck: 3-18               [1, 1024, 16, 28]         1,249,344\n",
       "│    │    └─SEResNetBottleneck: 3-19               [1, 1024, 16, 28]         1,249,344\n",
       "│    │    └─SEResNetBottleneck: 3-20               [1, 1024, 16, 28]         1,249,344\n",
       "│    │    └─SEResNetBottleneck: 3-21               [1, 1024, 16, 28]         1,249,344\n",
       "│    │    └─SEResNetBottleneck: 3-22               [1, 1024, 16, 28]         1,249,344\n",
       "│    │    └─SEResNetBottleneck: 3-23               [1, 1024, 16, 28]         1,249,344\n",
       "│    │    └─SEResNetBottleneck: 3-24               [1, 1024, 16, 28]         1,249,344\n",
       "│    │    └─SEResNetBottleneck: 3-25               [1, 1024, 16, 28]         1,249,344\n",
       "│    │    └─SEResNetBottleneck: 3-26               [1, 1024, 16, 28]         1,249,344\n",
       "│    │    └─SEResNetBottleneck: 3-27               [1, 1024, 16, 28]         1,249,344\n",
       "│    │    └─SEResNetBottleneck: 3-28               [1, 1024, 16, 28]         1,249,344\n",
       "│    │    └─SEResNetBottleneck: 3-29               [1, 1024, 16, 28]         1,249,344\n",
       "│    │    └─SEResNetBottleneck: 3-30               [1, 1024, 16, 28]         1,249,344\n",
       "│    │    └─SEResNetBottleneck: 3-31               [1, 1024, 16, 28]         1,249,344\n",
       "│    │    └─SEResNetBottleneck: 3-32               [1, 1024, 16, 28]         1,249,344\n",
       "│    │    └─SEResNetBottleneck: 3-33               [1, 1024, 16, 28]         1,249,344\n",
       "│    │    └─SEResNetBottleneck: 3-34               [1, 1024, 16, 28]         1,249,344\n",
       "│    │    └─SEResNetBottleneck: 3-35               [1, 1024, 16, 28]         1,249,344\n",
       "│    │    └─SEResNetBottleneck: 3-36               [1, 1024, 16, 28]         1,249,344\n",
       "│    │    └─SEResNetBottleneck: 3-37               [1, 1024, 16, 28]         1,249,344\n",
       "│    │    └─SEResNetBottleneck: 3-38               [1, 1024, 16, 28]         1,249,344\n",
       "│    │    └─SEResNetBottleneck: 3-39               [1, 1024, 16, 28]         1,249,344\n",
       "│    │    └─SEResNetBottleneck: 3-40               [1, 1024, 16, 28]         1,249,344\n",
       "│    │    └─SEResNetBottleneck: 3-41               [1, 1024, 16, 28]         1,249,344\n",
       "│    │    └─SEResNetBottleneck: 3-42               [1, 1024, 16, 28]         1,249,344\n",
       "│    │    └─SEResNetBottleneck: 3-43               [1, 1024, 16, 28]         1,249,344\n",
       "│    │    └─SEResNetBottleneck: 3-44               [1, 1024, 16, 28]         1,249,344\n",
       "│    │    └─SEResNetBottleneck: 3-45               [1, 1024, 16, 28]         1,249,344\n",
       "│    │    └─SEResNetBottleneck: 3-46               [1, 1024, 16, 28]         1,249,344\n",
       "│    │    └─SEResNetBottleneck: 3-47               [1, 1024, 16, 28]         1,249,344\n",
       "│    │    └─SEResNetBottleneck: 3-48               [1, 1024, 16, 28]         1,249,344\n",
       "│    │    └─SEResNetBottleneck: 3-49               [1, 1024, 16, 28]         1,249,344\n",
       "│    │    └─SEResNetBottleneck: 3-50               [1, 1024, 16, 28]         1,249,344\n",
       "│    │    └─SEResNetBottleneck: 3-51               [1, 1024, 16, 28]         1,249,344\n",
       "│    └─Sequential: 2-5                             [1, 2048, 8, 14]          --\n",
       "│    │    └─SEResNetBottleneck: 3-52               [1, 2048, 8, 14]          6,566,016\n",
       "│    │    └─SEResNetBottleneck: 3-53               [1, 2048, 8, 14]          4,989,056\n",
       "│    │    └─SEResNetBottleneck: 3-54               [1, 2048, 8, 14]          4,989,056\n",
       "├─UnetDecoder: 1-2                                 [1, 16, 256, 448]         --\n",
       "│    └─Identity: 2-6                               [1, 2048, 8, 14]          --\n",
       "│    └─ModuleList: 2-7                             --                        --\n",
       "│    │    └─DecoderBlock: 3-55                     [1, 256, 16, 28]          7,668,736\n",
       "│    │    └─DecoderBlock: 3-56                     [1, 128, 32, 56]          1,032,704\n",
       "│    │    └─DecoderBlock: 3-57                     [1, 64, 64, 112]          258,304\n",
       "│    │    └─DecoderBlock: 3-58                     [1, 32, 128, 224]         46,208\n",
       "│    │    └─DecoderBlock: 3-59                     [1, 16, 256, 448]         6,976\n",
       "├─SegmentationHead: 1-3                            [1, 1, 256, 448]          --\n",
       "│    └─Conv2d: 2-8                                 [1, 1, 256, 448]          145\n",
       "│    └─Identity: 2-9                               [1, 1, 256, 448]          --\n",
       "│    └─Activation: 2-10                            [1, 1, 256, 448]          --\n",
       "│    │    └─Sigmoid: 3-60                          [1, 1, 256, 448]          --\n",
       "====================================================================================================\n",
       "Total params: 73,785,921\n",
       "Trainable params: 73,785,921\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (G): 35.06\n",
       "====================================================================================================\n",
       "Input size (MB): 1.38\n",
       "Forward/backward pass size (MB): 920.66\n",
       "Params size (MB): 295.14\n",
       "Estimated Total Size (MB): 1217.18\n",
       "===================================================================================================="
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Path to training data\n",
    "img_path = './train_dataset/img'\n",
    "labels_path = './train_dataset/mask_img'\n",
    "\n",
    "# Model parameters\n",
    "ENCODER = ENCODER\n",
    "ENCODER_WEIGHTS = 'imagenet'\n",
    "ACTIVATION = 'sigmoid'\n",
    "\n",
    "# Constants\n",
    "INIT_LR = LR  # Initial learning rate\n",
    "NUM_EPOCHS = EPOCHS\n",
    "WARMUP_EPOCHS = int(NUM_EPOCHS * 0.1)  # Number of epochs for warmup\n",
    "MIN_LR = INIT_LR * DECAY  # Minimum learning rate after decay\n",
    "\n",
    "model = smp.Unet(\n",
    "    encoder_name=ENCODER, \n",
    "    encoder_weights=ENCODER_WEIGHTS, \n",
    "    classes=1, \n",
    "    activation=ACTIVATION,\n",
    ")\n",
    "\n",
    "preprocessing_fn = smp.encoders.get_preprocessing_fn(ENCODER, ENCODER_WEIGHTS)\n",
    "\n",
    "model.to(DEVICE)\n",
    "\n",
    "# Show model details\n",
    "summary(model, (1, 3, 256, 448))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "BATCH_SIZE = 32\n",
    "train_dataset, valid_dataset = load_split_dataset(img_path, labels_path, preprocessing_fn, valid_size=.2)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# Optimizer\n",
    "if OPTIM == 'adam':\n",
    "    optimizer = torch.optim.Adam([ \n",
    "        dict(params=model.parameters(), lr=INIT_LR, weight_decay=L2REG),\n",
    "    ])\n",
    "elif OPTIM == 'sgd':\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=INIT_LR, weight_decay=L2REG)\n",
    "\n",
    "# Loss and metrics\n",
    "loss = CombinedLoss(_eta=ETA)\n",
    "metrics = [utils.metrics.Fscore(beta=0.3 ** 0.5, threshold=0.5)]\n",
    "\n",
    "# Training and validation \n",
    "train_epoch = utils.train.TrainEpoch(\n",
    "    model,\n",
    "    loss=loss,\n",
    "    metrics=metrics,\n",
    "    optimizer=optimizer,\n",
    "    device=DEVICE,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "valid_epoch = utils.train.ValidEpoch(\n",
    "    model,\n",
    "    loss=loss,\n",
    "    metrics=metrics,\n",
    "    device=DEVICE,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 1, LR: 1e-06\n",
      "train: 100%|██████████| 108/108 [01:46<00:00,  1.02it/s, combined_loss - 2.212, fscore - 0.05212]\n",
      "valid: 100%|██████████| 27/27 [00:11<00:00,  2.39it/s, combined_loss - 2.186, fscore - 0.07023]\n",
      "Model saved at ./model[unet]_focal_enc[se_resnet152]_100eps_initlr[0.001]_decay[0.001]_bs[32]_wd[1e-05]_eta[1]_pn[0.4]_mccloss_BEST.pth!\n",
      "\n",
      "Epoch: 2, LR: 0.00010090000000000001\n",
      "train: 100%|██████████| 108/108 [01:47<00:00,  1.00it/s, combined_loss - 2.008, fscore - 0.2076]\n",
      "valid: 100%|██████████| 27/27 [00:10<00:00,  2.46it/s, combined_loss - 1.878, fscore - 0.3125]\n",
      "Model saved at ./model[unet]_focal_enc[se_resnet152]_100eps_initlr[0.001]_decay[0.001]_bs[32]_wd[1e-05]_eta[1]_pn[0.4]_mccloss_BEST.pth!\n",
      "\n",
      "Epoch: 3, LR: 0.00020080000000000003\n",
      "train: 100%|██████████| 108/108 [01:48<00:00,  1.01s/it, combined_loss - 1.548, fscore - 0.3892]\n",
      "valid: 100%|██████████| 27/27 [00:11<00:00,  2.38it/s, combined_loss - 0.9512, fscore - 0.515] \n",
      "Model saved at ./model[unet]_focal_enc[se_resnet152]_100eps_initlr[0.001]_decay[0.001]_bs[32]_wd[1e-05]_eta[1]_pn[0.4]_mccloss_BEST.pth!\n",
      "\n",
      "Epoch: 4, LR: 0.0003007000000000001\n",
      "train: 100%|██████████| 108/108 [01:48<00:00,  1.01s/it, combined_loss - 0.6231, fscore - 0.7181]\n",
      "valid: 100%|██████████| 27/27 [00:10<00:00,  2.50it/s, combined_loss - 0.5099, fscore - 0.7865]\n",
      "Model saved at ./model[unet]_focal_enc[se_resnet152]_100eps_initlr[0.001]_decay[0.001]_bs[32]_wd[1e-05]_eta[1]_pn[0.4]_mccloss_BEST.pth!\n",
      "\n",
      "Epoch: 5, LR: 0.0004006000000000001\n",
      "train: 100%|██████████| 108/108 [01:45<00:00,  1.03it/s, combined_loss - 0.5094, fscore - 0.7697]\n",
      "valid: 100%|██████████| 27/27 [00:13<00:00,  2.00it/s, combined_loss - 0.4893, fscore - 0.7851]\n",
      "\n",
      "Epoch: 6, LR: 0.0005005000000000001\n",
      "train: 100%|██████████| 108/108 [01:48<00:00,  1.00s/it, combined_loss - 0.4938, fscore - 0.7748]\n",
      "valid: 100%|██████████| 27/27 [00:11<00:00,  2.29it/s, combined_loss - 0.4731, fscore - 0.7885]\n",
      "Model saved at ./model[unet]_focal_enc[se_resnet152]_100eps_initlr[0.001]_decay[0.001]_bs[32]_wd[1e-05]_eta[1]_pn[0.4]_mccloss_BEST.pth!\n",
      "\n",
      "Epoch: 7, LR: 0.0006004000000000002\n",
      "train: 100%|██████████| 108/108 [01:47<00:00,  1.00it/s, combined_loss - 0.486, fscore - 0.7787] \n",
      "valid: 100%|██████████| 27/27 [00:11<00:00,  2.33it/s, combined_loss - 0.4742, fscore - 0.7897]\n",
      "Model saved at ./model[unet]_focal_enc[se_resnet152]_100eps_initlr[0.001]_decay[0.001]_bs[32]_wd[1e-05]_eta[1]_pn[0.4]_mccloss_BEST.pth!\n",
      "\n",
      "Epoch: 8, LR: 0.0007003000000000002\n",
      "train: 100%|██████████| 108/108 [01:50<00:00,  1.02s/it, combined_loss - 0.4866, fscore - 0.7758]\n",
      "valid: 100%|██████████| 27/27 [00:10<00:00,  2.49it/s, combined_loss - 0.4827, fscore - 0.779] \n",
      "\n",
      "Epoch: 9, LR: 0.0008002000000000002\n",
      "train: 100%|██████████| 108/108 [01:49<00:00,  1.02s/it, combined_loss - 0.4844, fscore - 0.7773]\n",
      "valid: 100%|██████████| 27/27 [00:12<00:00,  2.14it/s, combined_loss - 0.4684, fscore - 0.7816]\n",
      "\n",
      "Epoch: 10, LR: 0.0009001000000000001\n",
      "train: 100%|██████████| 108/108 [01:47<00:00,  1.00it/s, combined_loss - 0.4781, fscore - 0.7817]\n",
      "valid: 100%|██████████| 27/27 [00:11<00:00,  2.31it/s, combined_loss - 0.4652, fscore - 0.7879]\n",
      "\n",
      "Epoch: 11, LR: 0.001\n",
      "train: 100%|██████████| 108/108 [01:46<00:00,  1.01it/s, combined_loss - 0.4817, fscore - 0.7791]\n",
      "valid: 100%|██████████| 27/27 [00:12<00:00,  2.10it/s, combined_loss - 0.4653, fscore - 0.7833]\n",
      "\n",
      "Epoch: 12, LR: 0.0009996957180960385\n",
      "train: 100%|██████████| 108/108 [01:49<00:00,  1.01s/it, combined_loss - 0.4772, fscore - 0.7811]\n",
      "valid: 100%|██████████| 27/27 [00:13<00:00,  2.06it/s, combined_loss - 0.4744, fscore - 0.784] \n",
      "\n",
      "Epoch: 13, LR: 0.0009987832431047822\n",
      "train: 100%|██████████| 108/108 [01:47<00:00,  1.00it/s, combined_loss - 0.4722, fscore - 0.7841]\n",
      "valid: 100%|██████████| 27/27 [00:10<00:00,  2.48it/s, combined_loss - 0.4611, fscore - 0.7898]\n",
      "Model saved at ./model[unet]_focal_enc[se_resnet152]_100eps_initlr[0.001]_decay[0.001]_bs[32]_wd[1e-05]_eta[1]_pn[0.4]_mccloss_BEST.pth!\n",
      "\n",
      "Epoch: 14, LR: 0.0009972636867364526\n",
      "train: 100%|██████████| 108/108 [01:45<00:00,  1.02it/s, combined_loss - 0.4708, fscore - 0.7858]\n",
      "valid: 100%|██████████| 27/27 [00:12<00:00,  2.11it/s, combined_loss - 0.4636, fscore - 0.7932]\n",
      "Model saved at ./model[unet]_focal_enc[se_resnet152]_100eps_initlr[0.001]_decay[0.001]_bs[32]_wd[1e-05]_eta[1]_pn[0.4]_mccloss_BEST.pth!\n",
      "\n",
      "Epoch: 15, LR: 0.0009951389003364144\n",
      "train: 100%|██████████| 108/108 [01:47<00:00,  1.00it/s, combined_loss - 0.4687, fscore - 0.7868]\n",
      "valid: 100%|██████████| 27/27 [00:10<00:00,  2.48it/s, combined_loss - 0.4558, fscore - 0.794] \n",
      "Model saved at ./model[unet]_focal_enc[se_resnet152]_100eps_initlr[0.001]_decay[0.001]_bs[32]_wd[1e-05]_eta[1]_pn[0.4]_mccloss_BEST.pth!\n",
      "\n",
      "Epoch: 16, LR: 0.000992411472629598\n",
      "train: 100%|██████████| 108/108 [01:47<00:00,  1.00it/s, combined_loss - 0.4675, fscore - 0.7873]\n",
      "valid: 100%|██████████| 27/27 [00:11<00:00,  2.28it/s, combined_loss - 0.4559, fscore - 0.7941]\n",
      "Model saved at ./model[unet]_focal_enc[se_resnet152]_100eps_initlr[0.001]_decay[0.001]_bs[32]_wd[1e-05]_eta[1]_pn[0.4]_mccloss_BEST.pth!\n",
      "\n",
      "Epoch: 17, LR: 0.000989084726566536\n",
      "train: 100%|██████████| 108/108 [01:53<00:00,  1.05s/it, combined_loss - 0.463, fscore - 0.7909] \n",
      "valid: 100%|██████████| 27/27 [00:11<00:00,  2.43it/s, combined_loss - 0.4538, fscore - 0.7975]\n",
      "Model saved at ./model[unet]_focal_enc[se_resnet152]_100eps_initlr[0.001]_decay[0.001]_bs[32]_wd[1e-05]_eta[1]_pn[0.4]_mccloss_BEST.pth!\n",
      "\n",
      "Epoch: 18, LR: 0.00098516271527486\n",
      "train: 100%|██████████| 108/108 [01:51<00:00,  1.03s/it, combined_loss - 0.4647, fscore - 0.7887]\n",
      "valid: 100%|██████████| 27/27 [00:11<00:00,  2.37it/s, combined_loss - 0.4562, fscore - 0.7906]\n",
      "\n",
      "Epoch: 19, LR: 0.0009806502171211904\n",
      "train: 100%|██████████| 108/108 [01:48<00:00,  1.00s/it, combined_loss - 0.4634, fscore - 0.7904]\n",
      "valid: 100%|██████████| 27/27 [00:10<00:00,  2.53it/s, combined_loss - 0.461, fscore - 0.7935] \n",
      "\n",
      "Epoch: 20, LR: 0.0009755527298894294\n",
      "train: 100%|██████████| 108/108 [01:45<00:00,  1.03it/s, combined_loss - 0.4676, fscore - 0.7862]\n",
      "valid: 100%|██████████| 27/27 [00:10<00:00,  2.47it/s, combined_loss - 0.4576, fscore - 0.7902]\n",
      "\n",
      "Epoch: 21, LR: 0.0009698764640825614\n",
      "train: 100%|██████████| 108/108 [01:46<00:00,  1.01it/s, combined_loss - 0.4641, fscore - 0.7898]\n",
      "valid: 100%|██████████| 27/27 [00:11<00:00,  2.30it/s, combined_loss - 0.4547, fscore - 0.7956]\n",
      "\n",
      "Epoch: 22, LR: 0.0009636283353561104\n",
      "train: 100%|██████████| 108/108 [01:46<00:00,  1.01it/s, combined_loss - 0.4612, fscore - 0.7906]\n",
      "valid: 100%|██████████| 27/27 [00:10<00:00,  2.49it/s, combined_loss - 0.4539, fscore - 0.789] \n",
      "\n",
      "Epoch: 23, LR: 0.0009568159560924792\n",
      "train: 100%|██████████| 108/108 [01:48<00:00,  1.00s/it, combined_loss - 0.4593, fscore - 0.7918]\n",
      "valid: 100%|██████████| 27/27 [00:13<00:00,  2.06it/s, combined_loss - 0.458, fscore - 0.7862] \n",
      "\n",
      "Epoch: 24, LR: 0.0009494476261264341\n",
      "train: 100%|██████████| 108/108 [01:46<00:00,  1.02it/s, combined_loss - 0.4599, fscore - 0.7904]\n",
      "valid: 100%|██████████| 27/27 [00:15<00:00,  1.79it/s, combined_loss - 0.4479, fscore - 0.7989]\n",
      "Model saved at ./model[unet]_focal_enc[se_resnet152]_100eps_initlr[0.001]_decay[0.001]_bs[32]_wd[1e-05]_eta[1]_pn[0.4]_mccloss_BEST.pth!\n",
      "\n",
      "Epoch: 25, LR: 0.0009415323226330341\n",
      "train: 100%|██████████| 108/108 [01:52<00:00,  1.04s/it, combined_loss - 0.4611, fscore - 0.792] \n",
      "valid: 100%|██████████| 27/27 [00:13<00:00,  1.95it/s, combined_loss - 0.4558, fscore - 0.7938]\n",
      "\n",
      "Epoch: 26, LR: 0.0009330796891903273\n",
      "train: 100%|██████████| 108/108 [01:46<00:00,  1.02it/s, combined_loss - 0.4561, fscore - 0.7947]\n",
      "valid: 100%|██████████| 27/27 [00:12<00:00,  2.22it/s, combined_loss - 0.45, fscore - 0.7928]  \n",
      "\n",
      "Epoch: 27, LR: 0.0009241000240301348\n",
      "train: 100%|██████████| 108/108 [01:49<00:00,  1.02s/it, combined_loss - 0.4589, fscore - 0.7908]\n",
      "valid: 100%|██████████| 27/27 [00:11<00:00,  2.32it/s, combined_loss - 0.4523, fscore - 0.7963]\n",
      "\n",
      "Epoch: 28, LR: 0.0009146042674912434\n",
      "train: 100%|██████████| 108/108 [01:55<00:00,  1.07s/it, combined_loss - 0.4572, fscore - 0.7941]\n",
      "valid: 100%|██████████| 27/27 [00:11<00:00,  2.37it/s, combined_loss - 0.4521, fscore - 0.793] \n",
      "\n",
      "Epoch: 29, LR: 0.0009046039886902864\n",
      "train: 100%|██████████| 108/108 [01:55<00:00,  1.07s/it, combined_loss - 0.4565, fscore - 0.7936]\n",
      "valid: 100%|██████████| 27/27 [00:12<00:00,  2.21it/s, combined_loss - 0.4472, fscore - 0.7999]\n",
      "Model saved at ./model[unet]_focal_enc[se_resnet152]_100eps_initlr[0.001]_decay[0.001]_bs[32]_wd[1e-05]_eta[1]_pn[0.4]_mccloss_BEST.pth!\n",
      "\n",
      "Epoch: 31, LR: 0.0008831391993379295\n",
      "train: 100%|██████████| 108/108 [01:56<00:00,  1.08s/it, combined_loss - 0.4589, fscore - 0.7909]\n",
      "valid: 100%|██████████| 27/27 [00:14<00:00,  1.92it/s, combined_loss - 0.4519, fscore - 0.7911]\n",
      "\n",
      "Epoch: 32, LR: 0.0008717008403259585\n",
      "train: 100%|██████████| 108/108 [01:51<00:00,  1.04s/it, combined_loss - 0.454, fscore - 0.7952] \n",
      "valid: 100%|██████████| 27/27 [00:10<00:00,  2.50it/s, combined_loss - 0.4485, fscore - 0.7947]\n",
      "\n",
      "Epoch: 33, LR: 0.0008598102302691563\n",
      "train: 100%|██████████| 108/108 [01:49<00:00,  1.02s/it, combined_loss - 0.4564, fscore - 0.7928]\n",
      "valid: 100%|██████████| 27/27 [00:12<00:00,  2.15it/s, combined_loss - 0.451, fscore - 0.7924] \n",
      "\n",
      "Epoch: 34, LR: 0.0008474818560442692\n",
      "train: 100%|██████████| 108/108 [01:54<00:00,  1.06s/it, combined_loss - 0.4547, fscore - 0.7943]\n",
      "valid: 100%|██████████| 27/27 [00:18<00:00,  1.47it/s, combined_loss - 0.4488, fscore - 0.7963]\n",
      "\n",
      "Epoch: 35, LR: 0.0008347307378762498\n",
      "train: 100%|██████████| 108/108 [01:59<00:00,  1.11s/it, combined_loss - 0.4498, fscore - 0.7986]\n",
      "valid: 100%|██████████| 27/27 [00:12<00:00,  2.20it/s, combined_loss - 0.4472, fscore - 0.7996]\n",
      "\n",
      "Epoch: 36, LR: 0.0008215724110384265\n",
      "train: 100%|██████████| 108/108 [01:48<00:00,  1.01s/it, combined_loss - 0.4487, fscore - 0.7983]\n",
      "valid: 100%|██████████| 27/27 [00:10<00:00,  2.49it/s, combined_loss - 0.4451, fscore - 0.8008]\n",
      "Model saved at ./model[unet]_focal_enc[se_resnet152]_100eps_initlr[0.001]_decay[0.001]_bs[32]_wd[1e-05]_eta[1]_pn[0.4]_mccloss_BEST.pth!\n",
      "\n",
      "Epoch: 37, LR: 0.0008080229069251664\n",
      "train: 100%|██████████| 108/108 [01:58<00:00,  1.10s/it, combined_loss - 0.4509, fscore - 0.7963]\n",
      "valid: 100%|██████████| 27/27 [00:12<00:00,  2.17it/s, combined_loss - 0.446, fscore - 0.7959] \n",
      "\n",
      "Epoch: 38, LR: 0.0007940987335200904\n",
      "train: 100%|██████████| 108/108 [01:59<00:00,  1.10s/it, combined_loss - 0.4501, fscore - 0.7977]\n",
      "valid: 100%|██████████| 27/27 [00:13<00:00,  2.08it/s, combined_loss - 0.4409, fscore - 0.8015]\n",
      "Model saved at ./model[unet]_focal_enc[se_resnet152]_100eps_initlr[0.001]_decay[0.001]_bs[32]_wd[1e-05]_eta[1]_pn[0.4]_mccloss_BEST.pth!\n",
      "\n",
      "Epoch: 39, LR: 0.0007798168552836381\n",
      "train: 100%|██████████| 108/108 [01:51<00:00,  1.03s/it, combined_loss - 0.4475, fscore - 0.7983]\n",
      "valid: 100%|██████████| 27/27 [00:14<00:00,  1.84it/s, combined_loss - 0.443, fscore - 0.8011] \n",
      "\n",
      "Epoch: 40, LR: 0.0007651946724844859\n",
      "train: 100%|██████████| 108/108 [01:51<00:00,  1.03s/it, combined_loss - 0.4503, fscore - 0.7967]\n",
      "valid: 100%|██████████| 27/27 [00:13<00:00,  1.97it/s, combined_loss - 0.4426, fscore - 0.8049]\n",
      "Model saved at ./model[unet]_focal_enc[se_resnet152]_100eps_initlr[0.001]_decay[0.001]_bs[32]_wd[1e-05]_eta[1]_pn[0.4]_mccloss_BEST.pth!\n",
      "\n",
      "Epoch: 41, LR: 0.0007502500000000002\n",
      "train: 100%|██████████| 108/108 [01:50<00:00,  1.02s/it, combined_loss - 0.4478, fscore - 0.7989]\n",
      "valid: 100%|██████████| 27/27 [00:10<00:00,  2.51it/s, combined_loss - 0.4444, fscore - 0.803] \n",
      "\n",
      "Epoch: 42, LR: 0.0007350010456115525\n",
      "train: 100%|██████████| 108/108 [01:50<00:00,  1.03s/it, combined_loss - 0.4481, fscore - 0.7976]\n",
      "valid: 100%|██████████| 27/27 [00:12<00:00,  2.19it/s, combined_loss - 0.4428, fscore - 0.8012]\n",
      "\n",
      "Epoch: 43, LR: 0.0007194663878211442\n",
      "train: 100%|██████████| 108/108 [01:49<00:00,  1.02s/it, combined_loss - 0.4454, fscore - 0.7993]\n",
      "valid: 100%|██████████| 27/27 [00:11<00:00,  2.39it/s, combined_loss - 0.442, fscore - 0.7973] \n",
      "\n",
      "Epoch: 44, LR: 0.0007036649532163624\n",
      "train: 100%|██████████| 108/108 [01:47<00:00,  1.01it/s, combined_loss - 0.4443, fscore - 0.8007]\n",
      "valid: 100%|██████████| 27/27 [00:10<00:00,  2.48it/s, combined_loss - 0.4364, fscore - 0.8057]\n",
      "Model saved at ./model[unet]_focal_enc[se_resnet152]_100eps_initlr[0.001]_decay[0.001]_bs[32]_wd[1e-05]_eta[1]_pn[0.4]_mccloss_BEST.pth!\n",
      "\n",
      "Epoch: 45, LR: 0.0006876159934112483\n",
      "train: 100%|██████████| 108/108 [01:51<00:00,  1.03s/it, combined_loss - 0.4429, fscore - 0.802] \n",
      "valid: 100%|██████████| 27/27 [00:12<00:00,  2.13it/s, combined_loss - 0.4356, fscore - 0.8035]\n",
      "\n",
      "Epoch: 46, LR: 0.0006713390615911717\n",
      "train: 100%|██████████| 108/108 [01:49<00:00,  1.01s/it, combined_loss - 0.4448, fscore - 0.7992]\n",
      "valid: 100%|██████████| 27/27 [00:14<00:00,  1.89it/s, combined_loss - 0.4416, fscore - 0.802] \n",
      "\n",
      "Epoch: 47, LR: 0.0006548539886902864\n",
      "train: 100%|██████████| 108/108 [01:50<00:00,  1.02s/it, combined_loss - 0.4421, fscore - 0.8011]\n",
      "valid: 100%|██████████| 27/27 [00:12<00:00,  2.13it/s, combined_loss - 0.4375, fscore - 0.801] \n",
      "\n",
      "Epoch: 48, LR: 0.0006381808592305911\n",
      "train: 100%|██████████| 108/108 [01:56<00:00,  1.08s/it, combined_loss - 0.4417, fscore - 0.7998]\n",
      "valid: 100%|██████████| 27/27 [00:11<00:00,  2.36it/s, combined_loss - 0.4391, fscore - 0.797] \n",
      "\n",
      "Epoch: 49, LR: 0.0006213399868520341\n",
      "train: 100%|██████████| 108/108 [01:53<00:00,  1.05s/it, combined_loss - 0.4412, fscore - 0.8018]\n",
      "valid: 100%|██████████| 27/27 [00:14<00:00,  1.88it/s, combined_loss - 0.4358, fscore - 0.8048]\n",
      "\n",
      "Epoch: 50, LR: 0.0006043518895634708\n",
      "train: 100%|██████████| 108/108 [01:46<00:00,  1.02it/s, combined_loss - 0.4387, fscore - 0.803] \n",
      "valid: 100%|██████████| 27/27 [00:13<00:00,  2.00it/s, combined_loss - 0.4389, fscore - 0.7969]\n",
      "\n",
      "Epoch: 51, LR: 0.0005872372647446319\n",
      "train: 100%|██████████| 108/108 [01:53<00:00,  1.05s/it, combined_loss - 0.4369, fscore - 0.8039]\n",
      "valid: 100%|██████████| 27/27 [00:10<00:00,  2.47it/s, combined_loss - 0.4324, fscore - 0.8048]\n",
      "\n",
      "Epoch: 52, LR: 0.0005700169639295527\n",
      "train: 100%|██████████| 108/108 [01:46<00:00,  1.01it/s, combined_loss - 0.4345, fscore - 0.8064]\n",
      "valid: 100%|██████████| 27/27 [00:11<00:00,  2.30it/s, combined_loss - 0.4374, fscore - 0.7982]\n",
      "\n",
      "Epoch: 53, LR: 0.000552711967402193\n",
      "train: 100%|██████████| 108/108 [01:48<00:00,  1.01s/it, combined_loss - 0.4368, fscore - 0.8037]\n",
      "valid: 100%|██████████| 27/27 [00:10<00:00,  2.48it/s, combined_loss - 0.4345, fscore - 0.8043]\n",
      "\n",
      "Epoch: 54, LR: 0.0005353433586351906\n",
      "train: 100%|██████████| 108/108 [01:51<00:00,  1.03s/it, combined_loss - 0.4346, fscore - 0.805] \n",
      "valid: 100%|██████████| 27/27 [00:16<00:00,  1.65it/s, combined_loss - 0.4329, fscore - 0.8042]\n",
      "\n",
      "Epoch: 55, LR: 0.0005179322986028994\n",
      "train: 100%|██████████| 108/108 [01:56<00:00,  1.08s/it, combined_loss - 0.4363, fscore - 0.805] \n",
      "valid: 100%|██████████| 27/27 [00:11<00:00,  2.25it/s, combined_loss - 0.4375, fscore - 0.802] \n",
      "\n",
      "Epoch: 56, LR: 0.0005005000000000001\n",
      "train: 100%|██████████| 108/108 [01:49<00:00,  1.02s/it, combined_loss - 0.433, fscore - 0.8069] \n",
      "valid: 100%|██████████| 27/27 [00:11<00:00,  2.44it/s, combined_loss - 0.4318, fscore - 0.8043]\n",
      "\n",
      "Epoch: 57, LR: 0.000483067701397101\n",
      "train: 100%|██████████| 108/108 [01:49<00:00,  1.01s/it, combined_loss - 0.4327, fscore - 0.8064]\n",
      "valid: 100%|██████████| 27/27 [00:11<00:00,  2.36it/s, combined_loss - 0.433, fscore - 0.8035] \n",
      "\n",
      "Epoch: 58, LR: 0.00046565664136480946\n",
      "train: 100%|██████████| 108/108 [01:52<00:00,  1.04s/it, combined_loss - 0.4322, fscore - 0.8065]\n",
      "valid: 100%|██████████| 27/27 [00:15<00:00,  1.71it/s, combined_loss - 0.4324, fscore - 0.8027]\n",
      "\n",
      "Epoch: 59, LR: 0.00044828803259780724\n",
      "train: 100%|██████████| 108/108 [01:50<00:00,  1.03s/it, combined_loss - 0.4282, fscore - 0.8099]\n",
      "valid: 100%|██████████| 27/27 [00:12<00:00,  2.22it/s, combined_loss - 0.4294, fscore - 0.8036]\n",
      "\n",
      "Epoch: 60, LR: 0.0004309830360704475\n",
      "train: 100%|██████████| 108/108 [01:48<00:00,  1.00s/it, combined_loss - 0.4297, fscore - 0.8082]\n",
      "valid: 100%|██████████| 27/27 [00:10<00:00,  2.51it/s, combined_loss - 0.4287, fscore - 0.8029]\n",
      "\n",
      "Epoch: 61, LR: 0.0004137627352553684\n",
      "train: 100%|██████████| 108/108 [01:44<00:00,  1.03it/s, combined_loss - 0.4258, fscore - 0.8101]\n",
      "valid: 100%|██████████| 27/27 [00:10<00:00,  2.49it/s, combined_loss - 0.4289, fscore - 0.8044]\n",
      "\n",
      "Epoch: 62, LR: 0.0003966481104365294\n",
      "train: 100%|██████████| 108/108 [01:45<00:00,  1.03it/s, combined_loss - 0.4267, fscore - 0.8098]\n",
      "valid: 100%|██████████| 27/27 [00:11<00:00,  2.41it/s, combined_loss - 0.4263, fscore - 0.8083]\n",
      "Model saved at ./model[unet]_focal_enc[se_resnet152]_100eps_initlr[0.001]_decay[0.001]_bs[32]_wd[1e-05]_eta[1]_pn[0.4]_mccloss_BEST.pth!\n",
      "\n",
      "Epoch: 63, LR: 0.0003796600131479661\n",
      "train: 100%|██████████| 108/108 [01:49<00:00,  1.01s/it, combined_loss - 0.4242, fscore - 0.8116]\n",
      "valid: 100%|██████████| 27/27 [00:11<00:00,  2.32it/s, combined_loss - 0.4278, fscore - 0.8029]\n",
      "\n",
      "Epoch: 64, LR: 0.000362819140769409\n",
      "train: 100%|██████████| 108/108 [01:49<00:00,  1.01s/it, combined_loss - 0.4223, fscore - 0.813] \n",
      "valid: 100%|██████████| 27/27 [00:11<00:00,  2.42it/s, combined_loss - 0.4298, fscore - 0.7996]\n",
      "\n",
      "Epoch: 65, LR: 0.0003461460113097139\n",
      "train: 100%|██████████| 108/108 [01:50<00:00,  1.03s/it, combined_loss - 0.4218, fscore - 0.8125]\n",
      "valid: 100%|██████████| 27/27 [00:12<00:00,  2.18it/s, combined_loss - 0.4281, fscore - 0.8053]\n",
      "\n",
      "Epoch: 66, LR: 0.0003296609384088286\n",
      "train: 100%|██████████| 108/108 [01:51<00:00,  1.03s/it, combined_loss - 0.4207, fscore - 0.8134]\n",
      "valid: 100%|██████████| 27/27 [00:11<00:00,  2.25it/s, combined_loss - 0.4263, fscore - 0.8046]\n",
      "\n",
      "Epoch: 67, LR: 0.000313384006588752\n",
      "train: 100%|██████████| 108/108 [01:50<00:00,  1.03s/it, combined_loss - 0.4195, fscore - 0.8147]\n",
      "valid: 100%|██████████| 27/27 [00:13<00:00,  2.01it/s, combined_loss - 0.4252, fscore - 0.8059]\n",
      "\n",
      "Epoch: 68, LR: 0.0002973350467836379\n",
      "train: 100%|██████████| 108/108 [01:54<00:00,  1.06s/it, combined_loss - 0.4179, fscore - 0.8159]\n",
      "valid: 100%|██████████| 27/27 [00:12<00:00,  2.14it/s, combined_loss - 0.4256, fscore - 0.8052]\n",
      "\n",
      "Epoch: 69, LR: 0.00028153361217885583\n",
      "train: 100%|██████████| 108/108 [01:50<00:00,  1.03s/it, combined_loss - 0.4173, fscore - 0.8153]\n",
      "valid: 100%|██████████| 27/27 [00:14<00:00,  1.80it/s, combined_loss - 0.4265, fscore - 0.8049]\n",
      "\n",
      "Epoch: 70, LR: 0.00026599895438844776\n",
      "train: 100%|██████████| 108/108 [01:50<00:00,  1.02s/it, combined_loss - 0.4164, fscore - 0.8154]\n",
      "valid: 100%|██████████| 27/27 [00:11<00:00,  2.26it/s, combined_loss - 0.4246, fscore - 0.8059]\n",
      "\n",
      "Epoch: 71, LR: 0.00025075000000000016\n",
      "train: 100%|██████████| 108/108 [01:47<00:00,  1.00it/s, combined_loss - 0.4155, fscore - 0.8166]\n",
      "valid: 100%|██████████| 27/27 [00:11<00:00,  2.38it/s, combined_loss - 0.4256, fscore - 0.805] \n",
      "\n",
      "Epoch: 72, LR: 0.00023580532751551423\n",
      "train: 100%|██████████| 108/108 [01:54<00:00,  1.06s/it, combined_loss - 0.4145, fscore - 0.8164]\n",
      "valid: 100%|██████████| 27/27 [00:12<00:00,  2.21it/s, combined_loss - 0.4253, fscore - 0.8057]\n",
      "\n",
      "Epoch: 73, LR: 0.00022118314471636206\n",
      "train: 100%|██████████| 108/108 [01:47<00:00,  1.00it/s, combined_loss - 0.4134, fscore - 0.8177]\n",
      "valid: 100%|██████████| 27/27 [00:11<00:00,  2.42it/s, combined_loss - 0.4247, fscore - 0.8063]\n",
      "\n",
      "Epoch: 74, LR: 0.00020690126647990973\n",
      "train: 100%|██████████| 108/108 [01:47<00:00,  1.00it/s, combined_loss - 0.4115, fscore - 0.8187]\n",
      "valid: 100%|██████████| 27/27 [00:11<00:00,  2.42it/s, combined_loss - 0.4239, fscore - 0.8058]\n",
      "\n",
      "Epoch: 75, LR: 0.0001929770930748337\n",
      "train: 100%|██████████| 108/108 [01:48<00:00,  1.00s/it, combined_loss - 0.4114, fscore - 0.8194]\n",
      "valid: 100%|██████████| 27/27 [00:11<00:00,  2.38it/s, combined_loss - 0.4248, fscore - 0.8037]\n",
      "\n",
      "Epoch: 76, LR: 0.0001794275889615736\n",
      "train: 100%|██████████| 108/108 [01:48<00:00,  1.00s/it, combined_loss - 0.4095, fscore - 0.8203]\n",
      "valid: 100%|██████████| 27/27 [00:12<00:00,  2.18it/s, combined_loss - 0.4244, fscore - 0.8022]\n",
      "\n",
      "Epoch: 77, LR: 0.0001662692621237505\n",
      "train: 100%|██████████| 108/108 [01:48<00:00,  1.01s/it, combined_loss - 0.4079, fscore - 0.8217]\n",
      "valid: 100%|██████████| 27/27 [00:11<00:00,  2.26it/s, combined_loss - 0.4232, fscore - 0.806] \n",
      "\n",
      "Epoch: 78, LR: 0.00015351814395573083\n",
      "train: 100%|██████████| 108/108 [01:48<00:00,  1.01s/it, combined_loss - 0.4057, fscore - 0.8235]\n",
      "valid: 100%|██████████| 27/27 [00:13<00:00,  2.01it/s, combined_loss - 0.4235, fscore - 0.8041]\n",
      "\n",
      "Epoch: 79, LR: 0.00014118976973084385\n",
      "train: 100%|██████████| 108/108 [01:48<00:00,  1.00s/it, combined_loss - 0.407, fscore - 0.8223] \n",
      "valid: 100%|██████████| 27/27 [00:11<00:00,  2.33it/s, combined_loss - 0.4249, fscore - 0.8022]\n",
      "\n",
      "Epoch: 80, LR: 0.00012929915967404152\n",
      "train: 100%|██████████| 108/108 [01:50<00:00,  1.02s/it, combined_loss - 0.4045, fscore - 0.824] \n",
      "valid: 100%|██████████| 27/27 [00:11<00:00,  2.37it/s, combined_loss - 0.4236, fscore - 0.8045]\n",
      "\n",
      "Epoch: 81, LR: 0.00011786080066207055\n",
      "train:  46%|████▋     | 50/108 [00:51<01:08,  1.19s/it, combined_loss - 0.4057, fscore - 0.8234]"
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "max_score = 0\n",
    "cnt = 0\n",
    "\n",
    "# Initialize lists to store the F-scores for plotting later\n",
    "train_f_scores = []\n",
    "valid_f_scores = []\n",
    "\n",
    "for i in range(NUM_EPOCHS):\n",
    "    current_lr = get_lr(i, WARMUP_EPOCHS, NUM_EPOCHS, INIT_LR, MIN_LR)\n",
    "    # Update optimizer with the current learning rate\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = current_lr\n",
    "    \n",
    "    print(f'\\nEpoch: {i+1}, LR: {current_lr}')\n",
    "    train_logs = train_epoch.run(train_loader)\n",
    "    valid_logs = valid_epoch.run(valid_loader)\n",
    "\n",
    "    # Record the F-scores\n",
    "    train_f_scores.append(train_logs['fscore'])\n",
    "    valid_f_scores.append(valid_logs['fscore'])\n",
    "\n",
    "    # Save model on best validation score\n",
    "    if max_score < valid_logs['fscore']:\n",
    "        cnt = i\n",
    "        max_score = valid_logs['fscore']\n",
    "        model_path = f'./model[{model.__class__.__name__.lower()}]_focal_enc[{ENCODER}]_{NUM_EPOCHS}eps_initlr[{INIT_LR}]_decay[{DECAY}]_bs[{BATCH_SIZE}]_wd[{L2REG}]_eta[{ETA}]_pn[{PROB}]_mccloss_BEST.pth'\n",
    "        torch.save(model, model_path)\n",
    "        print(f'Model saved at {model_path}!')\n",
    "# Final model save\n",
    "final_model_path = f'./model[{model.__class__.__name__.lower()}]_focal_enc[{ENCODER}]_{NUM_EPOCHS}eps_initlr[{INIT_LR}]_decay[{DECAY}]_bs[{BATCH_SIZE}]_wd[{L2REG}]_eta[{ETA}]_pn[{PROB}]_mccloss.pth'\n",
    "torch.save(model, final_model_path)\n",
    "print('Training completed!')\n",
    "print(f\"Best Model @ {cnt+1} epochs; F-Score: {max_score}, for {model.__class__.__name__.lower()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save train config\n",
    "config = {\n",
    "    'Model': model.__class__.__name__.lower(),\n",
    "    'Encoder': ENCODER,\n",
    "    'Initial Learning Rate': INIT_LR,\n",
    "    'Decay': DECAY,\n",
    "    'Number of Epochs': NUM_EPOCHS,\n",
    "    'Optimizer': OPTIM,\n",
    "    'L2 regularization': L2REG,\n",
    "    'Batch Size': BATCH_SIZE,\n",
    "    'Eta': ETA,\n",
    "    'Loss': 'Focal + MCCLoss + SSIMLoss',\n",
    "    'Probability for blurring and noise': PROB\n",
    "}\n",
    "\n",
    "# Find the final validation F-score\n",
    "final_val_fscore = valid_f_scores[-1] if valid_f_scores else 0\n",
    "\n",
    "# Log the results to a file\n",
    "log_results(f\"{model.__class__.__name__.lower()}_training_results.txt\", config, max_score, cnt, final_val_fscore)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NavImgDataset(Dataset):\n",
    "    def __init__(self, img_paths, preprocessing=None, augmentation=None):\n",
    "        self.img_paths = img_paths\n",
    "        self.preprocessing = preprocessing\n",
    "        self.augmentation = augmentation\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_paths)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img = cv2.imread(self.img_paths[index])\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        if self.augmentation:\n",
    "            sample = self.augmentation(image=img, mask=img)\n",
    "            img, _ = sample['image'], sample['mask']\n",
    "        if self.preprocessing:\n",
    "            sample = self.preprocessing(image=img, mask=img)\n",
    "            img, _ = sample['image'], sample['mask']\n",
    "        return img\n",
    "\n",
    "def get_preprocessing(preprocessing_fn):    \n",
    "    _transform = [\n",
    "        albu.Lambda(image=preprocessing_fn),\n",
    "        albu.Lambda(image=to_tensor, mask=to_tensor),\n",
    "    ]\n",
    "    return albu.Compose(_transform)\n",
    "\n",
    "def get_testing_augmentation():\n",
    "    test_transform = [\n",
    "        albu.PadIfNeeded(min_height=256, min_width=448, always_apply=True, border_mode=0),\n",
    "        albu.CLAHE(clip_limit=2.0, tile_grid_size=(8, 8), always_apply=True)\n",
    "    ]\n",
    "    return albu.Compose(test_transform)\n",
    "\n",
    "def to_tensor(x, **kwargs):\n",
    "    return x.transpose(2, 0, 1).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/raid/jupyter-rick.109101013.md0-1ced0/.conda/envs/seg/lib/python3.9/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "====================================================================================================\n",
       "Layer (type:depth-idx)                             Output Shape              Param #\n",
       "====================================================================================================\n",
       "Unet                                               [1, 1, 256, 448]          --\n",
       "├─SENetEncoder: 1-1                                [1, 3, 256, 448]          --\n",
       "│    └─Sequential: 2-1                             --                        --\n",
       "│    │    └─Conv2d: 3-1                            [1, 64, 128, 224]         9,408\n",
       "│    │    └─BatchNorm2d: 3-2                       [1, 64, 128, 224]         128\n",
       "│    │    └─ReLU: 3-3                              [1, 64, 128, 224]         --\n",
       "│    │    └─MaxPool2d: 3-4                         [1, 64, 64, 112]          --\n",
       "│    └─Sequential: 2-2                             [1, 256, 64, 112]         --\n",
       "│    │    └─SEResNetBottleneck: 3-5                [1, 256, 64, 112]         83,472\n",
       "│    │    └─SEResNetBottleneck: 3-6                [1, 256, 64, 112]         78,864\n",
       "│    │    └─SEResNetBottleneck: 3-7                [1, 256, 64, 112]         78,864\n",
       "│    └─Sequential: 2-3                             [1, 512, 32, 56]          --\n",
       "│    │    └─SEResNetBottleneck: 3-8                [1, 512, 32, 56]          412,704\n",
       "│    │    └─SEResNetBottleneck: 3-9                [1, 512, 32, 56]          313,376\n",
       "│    │    └─SEResNetBottleneck: 3-10               [1, 512, 32, 56]          313,376\n",
       "│    │    └─SEResNetBottleneck: 3-11               [1, 512, 32, 56]          313,376\n",
       "│    │    └─SEResNetBottleneck: 3-12               [1, 512, 32, 56]          313,376\n",
       "│    │    └─SEResNetBottleneck: 3-13               [1, 512, 32, 56]          313,376\n",
       "│    │    └─SEResNetBottleneck: 3-14               [1, 512, 32, 56]          313,376\n",
       "│    │    └─SEResNetBottleneck: 3-15               [1, 512, 32, 56]          313,376\n",
       "│    └─Sequential: 2-4                             [1, 1024, 16, 28]         --\n",
       "│    │    └─SEResNetBottleneck: 3-16               [1, 1024, 16, 28]         1,644,608\n",
       "│    │    └─SEResNetBottleneck: 3-17               [1, 1024, 16, 28]         1,249,344\n",
       "│    │    └─SEResNetBottleneck: 3-18               [1, 1024, 16, 28]         1,249,344\n",
       "│    │    └─SEResNetBottleneck: 3-19               [1, 1024, 16, 28]         1,249,344\n",
       "│    │    └─SEResNetBottleneck: 3-20               [1, 1024, 16, 28]         1,249,344\n",
       "│    │    └─SEResNetBottleneck: 3-21               [1, 1024, 16, 28]         1,249,344\n",
       "│    │    └─SEResNetBottleneck: 3-22               [1, 1024, 16, 28]         1,249,344\n",
       "│    │    └─SEResNetBottleneck: 3-23               [1, 1024, 16, 28]         1,249,344\n",
       "│    │    └─SEResNetBottleneck: 3-24               [1, 1024, 16, 28]         1,249,344\n",
       "│    │    └─SEResNetBottleneck: 3-25               [1, 1024, 16, 28]         1,249,344\n",
       "│    │    └─SEResNetBottleneck: 3-26               [1, 1024, 16, 28]         1,249,344\n",
       "│    │    └─SEResNetBottleneck: 3-27               [1, 1024, 16, 28]         1,249,344\n",
       "│    │    └─SEResNetBottleneck: 3-28               [1, 1024, 16, 28]         1,249,344\n",
       "│    │    └─SEResNetBottleneck: 3-29               [1, 1024, 16, 28]         1,249,344\n",
       "│    │    └─SEResNetBottleneck: 3-30               [1, 1024, 16, 28]         1,249,344\n",
       "│    │    └─SEResNetBottleneck: 3-31               [1, 1024, 16, 28]         1,249,344\n",
       "│    │    └─SEResNetBottleneck: 3-32               [1, 1024, 16, 28]         1,249,344\n",
       "│    │    └─SEResNetBottleneck: 3-33               [1, 1024, 16, 28]         1,249,344\n",
       "│    │    └─SEResNetBottleneck: 3-34               [1, 1024, 16, 28]         1,249,344\n",
       "│    │    └─SEResNetBottleneck: 3-35               [1, 1024, 16, 28]         1,249,344\n",
       "│    │    └─SEResNetBottleneck: 3-36               [1, 1024, 16, 28]         1,249,344\n",
       "│    │    └─SEResNetBottleneck: 3-37               [1, 1024, 16, 28]         1,249,344\n",
       "│    │    └─SEResNetBottleneck: 3-38               [1, 1024, 16, 28]         1,249,344\n",
       "│    │    └─SEResNetBottleneck: 3-39               [1, 1024, 16, 28]         1,249,344\n",
       "│    │    └─SEResNetBottleneck: 3-40               [1, 1024, 16, 28]         1,249,344\n",
       "│    │    └─SEResNetBottleneck: 3-41               [1, 1024, 16, 28]         1,249,344\n",
       "│    │    └─SEResNetBottleneck: 3-42               [1, 1024, 16, 28]         1,249,344\n",
       "│    │    └─SEResNetBottleneck: 3-43               [1, 1024, 16, 28]         1,249,344\n",
       "│    │    └─SEResNetBottleneck: 3-44               [1, 1024, 16, 28]         1,249,344\n",
       "│    │    └─SEResNetBottleneck: 3-45               [1, 1024, 16, 28]         1,249,344\n",
       "│    │    └─SEResNetBottleneck: 3-46               [1, 1024, 16, 28]         1,249,344\n",
       "│    │    └─SEResNetBottleneck: 3-47               [1, 1024, 16, 28]         1,249,344\n",
       "│    │    └─SEResNetBottleneck: 3-48               [1, 1024, 16, 28]         1,249,344\n",
       "│    │    └─SEResNetBottleneck: 3-49               [1, 1024, 16, 28]         1,249,344\n",
       "│    │    └─SEResNetBottleneck: 3-50               [1, 1024, 16, 28]         1,249,344\n",
       "│    │    └─SEResNetBottleneck: 3-51               [1, 1024, 16, 28]         1,249,344\n",
       "│    └─Sequential: 2-5                             [1, 2048, 8, 14]          --\n",
       "│    │    └─SEResNetBottleneck: 3-52               [1, 2048, 8, 14]          6,566,016\n",
       "│    │    └─SEResNetBottleneck: 3-53               [1, 2048, 8, 14]          4,989,056\n",
       "│    │    └─SEResNetBottleneck: 3-54               [1, 2048, 8, 14]          4,989,056\n",
       "├─UnetDecoder: 1-2                                 [1, 16, 256, 448]         --\n",
       "│    └─Identity: 2-6                               [1, 2048, 8, 14]          --\n",
       "│    └─ModuleList: 2-7                             --                        --\n",
       "│    │    └─DecoderBlock: 3-55                     [1, 256, 16, 28]          7,668,736\n",
       "│    │    └─DecoderBlock: 3-56                     [1, 128, 32, 56]          1,032,704\n",
       "│    │    └─DecoderBlock: 3-57                     [1, 64, 64, 112]          258,304\n",
       "│    │    └─DecoderBlock: 3-58                     [1, 32, 128, 224]         46,208\n",
       "│    │    └─DecoderBlock: 3-59                     [1, 16, 256, 448]         6,976\n",
       "├─SegmentationHead: 1-3                            [1, 1, 256, 448]          --\n",
       "│    └─Conv2d: 2-8                                 [1, 1, 256, 448]          145\n",
       "│    └─Identity: 2-9                               [1, 1, 256, 448]          --\n",
       "│    └─Activation: 2-10                            [1, 1, 256, 448]          --\n",
       "│    │    └─Sigmoid: 3-60                          [1, 1, 256, 448]          --\n",
       "====================================================================================================\n",
       "Total params: 73,785,921\n",
       "Trainable params: 73,785,921\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (G): 35.06\n",
       "====================================================================================================\n",
       "Input size (MB): 1.38\n",
       "Forward/backward pass size (MB): 920.66\n",
       "Params size (MB): 295.14\n",
       "Estimated Total Size (MB): 1217.18\n",
       "===================================================================================================="
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model\n",
    "ENCODER = 'se_resnet152'\n",
    "ENCODER_WEIGHTS = 'imagenet'\n",
    "ACTIVATION = 'sigmoid'\n",
    "\n",
    "model = smp.Unet(\n",
    "    encoder_name=ENCODER, \n",
    "    encoder_weights=None, \n",
    "    classes=1, \n",
    "    activation=ACTIVATION,\n",
    ")\n",
    "\n",
    "preprocessing_fn = smp.encoders.get_preprocessing_fn(ENCODER, ENCODER_WEIGHTS)\n",
    "model_path = 'model[unet]_focal_enc[se_resnet152]_100eps_initlr[0.001]_decay[0.001]_bs[32]_wd[1e-05]_eta[1]_pn[0.4]_mccloss_BEST.pth'\n",
    "model = torch.load(model_path)\n",
    "model.to(DEVICE)\n",
    "summary(model, (1, 3, 256, 448))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory already exists at ./test_results_model[unet]_focal_enc[se_resnet152]_100eps_initlr[0.001]_decay[0.001]_bs[32]_wd[1e-05]_eta[1]_pn[0.4]_mccloss_BEST\n",
      "Processed PRI_RI_2000000.\n",
      "Processed PRI_RI_2000001.\n",
      "Processed PRI_RI_2000002.\n",
      "Processed PRI_RI_2000003.\n",
      "Processed PRI_RI_2000004.\n",
      "Processed PRI_RI_2000005.\n",
      "Processed PRI_RI_2000006.\n",
      "Processed PRI_RI_2000007.\n",
      "Processed PRI_RI_2000008.\n",
      "Processed PRI_RI_2000009.\n",
      "Processed PRI_RI_2000010.\n",
      "Processed PRI_RI_2000011.\n",
      "Processed PRI_RI_2000012.\n",
      "Processed PRI_RI_2000013.\n",
      "Processed PRI_RI_2000014.\n",
      "Processed PRI_RI_2000015.\n",
      "Processed PRI_RI_2000016.\n",
      "Processed PRI_RI_2000017.\n",
      "Processed PRI_RI_2000018.\n",
      "Processed PRI_RI_2000019.\n",
      "Processed PRI_RI_2000020.\n",
      "Processed PRI_RI_2000021.\n",
      "Processed PRI_RI_2000022.\n",
      "Processed PRI_RI_2000023.\n",
      "Processed PRI_RI_2000024.\n",
      "Processed PRI_RI_2000025.\n",
      "Processed PRI_RI_2000026.\n",
      "Processed PRI_RI_2000027.\n",
      "Processed PRI_RI_2000028.\n",
      "Processed PRI_RI_2000029.\n",
      "Processed PRI_RI_2000030.\n",
      "Processed PRI_RI_2000031.\n",
      "Processed PRI_RI_2000032.\n",
      "Processed PRI_RI_2000033.\n",
      "Processed PRI_RI_2000034.\n",
      "Processed PRI_RI_2000035.\n",
      "Processed PRI_RI_2000036.\n",
      "Processed PRI_RI_2000037.\n",
      "Processed PRI_RI_2000038.\n",
      "Processed PRI_RI_2000039.\n",
      "Processed PRI_RI_2000040.\n",
      "Processed PRI_RI_2000041.\n",
      "Processed PRI_RI_2000042.\n",
      "Processed PRI_RI_2000043.\n",
      "Processed PRI_RI_2000044.\n",
      "Processed PRI_RI_2000045.\n",
      "Processed PRI_RI_2000046.\n",
      "Processed PRI_RI_2000047.\n",
      "Processed PRI_RI_2000048.\n",
      "Processed PRI_RI_2000049.\n",
      "Processed PRI_RI_2000050.\n",
      "Processed PRI_RI_2000051.\n",
      "Processed PRI_RI_2000052.\n",
      "Processed PRI_RI_2000053.\n",
      "Processed PRI_RI_2000054.\n",
      "Processed PRI_RI_2000055.\n",
      "Processed PRI_RI_2000056.\n",
      "Processed PRI_RI_2000057.\n",
      "Processed PRI_RI_2000058.\n",
      "Processed PRI_RI_2000059.\n",
      "Processed PRI_RI_2000060.\n",
      "Processed PRI_RI_2000061.\n",
      "Processed PRI_RI_2000062.\n",
      "Processed PRI_RI_2000063.\n",
      "Processed PRI_RI_2000064.\n",
      "Processed PRI_RI_2000065.\n",
      "Processed PRI_RI_2000066.\n",
      "Processed PRI_RI_2000067.\n",
      "Processed PRI_RI_2000068.\n",
      "Processed PRI_RI_2000069.\n",
      "Processed PRI_RI_2000070.\n",
      "Processed PRI_RI_2000071.\n",
      "Processed PRI_RI_2000072.\n",
      "Processed PRI_RI_2000073.\n",
      "Processed PRI_RI_2000074.\n",
      "Processed PRI_RI_2000075.\n",
      "Processed PRI_RI_2000076.\n",
      "Processed PRI_RI_2000077.\n",
      "Processed PRI_RI_2000078.\n",
      "Processed PRI_RI_2000079.\n",
      "Processed PRI_RI_2000080.\n",
      "Processed PRI_RI_2000081.\n",
      "Processed PRI_RI_2000082.\n",
      "Processed PRI_RI_2000083.\n",
      "Processed PRI_RI_2000084.\n",
      "Processed PRI_RI_2000085.\n",
      "Processed PRI_RI_2000086.\n",
      "Processed PRI_RI_2000087.\n",
      "Processed PRI_RI_2000088.\n",
      "Processed PRI_RI_2000089.\n",
      "Processed PRI_RI_2000090.\n",
      "Processed PRI_RI_2000091.\n",
      "Processed PRI_RI_2000092.\n",
      "Processed PRI_RI_2000093.\n",
      "Processed PRI_RI_2000094.\n",
      "Processed PRI_RI_2000095.\n",
      "Processed PRI_RI_2000096.\n",
      "Processed PRI_RI_2000097.\n",
      "Processed PRI_RI_2000098.\n",
      "Processed PRI_RI_2000099.\n",
      "Processed PRI_RI_2000100.\n",
      "Processed PRI_RI_2000101.\n",
      "Processed PRI_RI_2000102.\n",
      "Processed PRI_RI_2000103.\n",
      "Processed PRI_RI_2000104.\n",
      "Processed PRI_RI_2000105.\n",
      "Processed PRI_RI_2000106.\n",
      "Processed PRI_RI_2000107.\n",
      "Processed PRI_RI_2000108.\n",
      "Processed PRI_RI_2000109.\n",
      "Processed PRI_RI_2000110.\n",
      "Processed PRI_RI_2000111.\n",
      "Processed PRI_RI_2000112.\n",
      "Processed PRI_RI_2000113.\n",
      "Processed PRI_RI_2000114.\n",
      "Processed PRI_RI_2000115.\n",
      "Processed PRI_RI_2000116.\n",
      "Processed PRI_RI_2000117.\n",
      "Processed PRI_RI_2000118.\n",
      "Processed PRI_RI_2000119.\n",
      "Processed PRI_RI_2000120.\n",
      "Processed PRI_RI_2000121.\n",
      "Processed PRI_RI_2000122.\n",
      "Processed PRI_RI_2000123.\n",
      "Processed PRI_RI_2000124.\n",
      "Processed PRI_RI_2000125.\n",
      "Processed PRI_RI_2000126.\n",
      "Processed PRI_RI_2000127.\n",
      "Processed PRI_RI_2000128.\n",
      "Processed PRI_RI_2000129.\n",
      "Processed PRI_RI_2000130.\n",
      "Processed PRI_RI_2000131.\n",
      "Processed PRI_RI_2000132.\n",
      "Processed PRI_RI_2000133.\n",
      "Processed PRI_RI_2000134.\n",
      "Processed PRI_RI_2000135.\n",
      "Processed PRI_RI_2000136.\n",
      "Processed PRI_RI_2000137.\n",
      "Processed PRI_RI_2000138.\n",
      "Processed PRI_RI_2000139.\n",
      "Processed PRI_RI_2000140.\n",
      "Processed PRI_RI_2000141.\n",
      "Processed PRI_RI_2000142.\n",
      "Processed PRI_RI_2000143.\n",
      "Processed PRI_RI_2000144.\n",
      "Processed PRI_RI_2000145.\n",
      "Processed PRI_RI_2000146.\n",
      "Processed PRI_RI_2000147.\n",
      "Processed PRI_RI_2000148.\n",
      "Processed PRI_RI_2000149.\n",
      "Processed PRI_RI_2000150.\n",
      "Processed PRI_RI_2000151.\n",
      "Processed PRI_RI_2000152.\n",
      "Processed PRI_RI_2000153.\n",
      "Processed PRI_RI_2000154.\n",
      "Processed PRI_RI_2000155.\n",
      "Processed PRI_RI_2000156.\n",
      "Processed PRI_RI_2000157.\n",
      "Processed PRI_RI_2000158.\n",
      "Processed PRI_RI_2000159.\n",
      "Processed PRI_RI_2000160.\n",
      "Processed PRI_RI_2000161.\n",
      "Processed PRI_RI_2000162.\n",
      "Processed PRI_RI_2000163.\n",
      "Processed PRI_RI_2000164.\n",
      "Processed PRI_RI_2000165.\n",
      "Processed PRI_RI_2000166.\n",
      "Processed PRI_RI_2000167.\n",
      "Processed PRI_RI_2000168.\n",
      "Processed PRI_RI_2000169.\n",
      "Processed PRI_RI_2000170.\n",
      "Processed PRI_RI_2000171.\n",
      "Processed PRI_RI_2000172.\n",
      "Processed PRI_RI_2000173.\n",
      "Processed PRI_RI_2000174.\n",
      "Processed PRI_RI_2000175.\n",
      "Processed PRI_RI_2000176.\n",
      "Processed PRI_RI_2000177.\n",
      "Processed PRI_RI_2000178.\n",
      "Processed PRI_RI_2000179.\n",
      "Processed PRI_RI_2000180.\n",
      "Processed PRI_RI_2000181.\n",
      "Processed PRI_RI_2000182.\n",
      "Processed PRI_RI_2000183.\n",
      "Processed PRI_RI_2000184.\n",
      "Processed PRI_RI_2000185.\n",
      "Processed PRI_RI_2000186.\n",
      "Processed PRI_RI_2000187.\n",
      "Processed PRI_RI_2000188.\n",
      "Processed PRI_RI_2000189.\n",
      "Processed PRI_RI_2000190.\n",
      "Processed PRI_RI_2000191.\n",
      "Processed PRI_RI_2000192.\n",
      "Processed PRI_RI_2000193.\n",
      "Processed PRI_RI_2000194.\n",
      "Processed PRI_RI_2000195.\n",
      "Processed PRI_RI_2000196.\n",
      "Processed PRI_RI_2000197.\n",
      "Processed PRI_RI_2000198.\n",
      "Processed PRI_RI_2000199.\n",
      "Processed PRI_RI_2000200.\n",
      "Processed PRI_RI_2000201.\n",
      "Processed PRI_RI_2000202.\n",
      "Processed PRI_RI_2000203.\n",
      "Processed PRI_RI_2000204.\n",
      "Processed PRI_RI_2000205.\n",
      "Processed PRI_RI_2000206.\n",
      "Processed PRI_RI_2000207.\n",
      "Processed PRI_RI_2000208.\n",
      "Processed PRI_RI_2000209.\n",
      "Processed PRI_RI_2000210.\n",
      "Processed PRI_RI_2000211.\n",
      "Processed PRI_RI_2000212.\n",
      "Processed PRI_RI_2000213.\n",
      "Processed PRI_RI_2000214.\n",
      "Processed PRI_RI_2000215.\n",
      "Processed PRI_RI_2000216.\n",
      "Processed PRI_RI_2000217.\n",
      "Processed PRI_RI_2000218.\n",
      "Processed PRI_RI_2000219.\n",
      "Processed PRI_RI_2000220.\n",
      "Processed PRI_RI_2000221.\n",
      "Processed PRI_RI_2000222.\n",
      "Processed PRI_RI_2000223.\n",
      "Processed PRI_RI_2000224.\n",
      "Processed PRI_RI_2000225.\n",
      "Processed PRI_RI_2000226.\n",
      "Processed PRI_RI_2000227.\n",
      "Processed PRI_RI_2000228.\n",
      "Processed PRI_RI_2000229.\n",
      "Processed PRI_RI_2000230.\n",
      "Processed PRI_RI_2000231.\n",
      "Processed PRI_RI_2000232.\n",
      "Processed PRI_RI_2000233.\n",
      "Processed PRI_RI_2000234.\n",
      "Processed PRI_RI_2000235.\n",
      "Processed PRI_RI_2000236.\n",
      "Processed PRI_RI_2000237.\n",
      "Processed PRI_RI_2000238.\n",
      "Processed PRI_RI_2000239.\n",
      "Processed PRI_RI_2000240.\n",
      "Processed PRI_RI_2000241.\n",
      "Processed PRI_RI_2000242.\n",
      "Processed PRI_RI_2000243.\n",
      "Processed PRI_RI_2000244.\n",
      "Processed PRI_RI_2000245.\n",
      "Processed PRI_RI_2000246.\n",
      "Processed PRI_RI_2000247.\n",
      "Processed PRI_RI_2000248.\n",
      "Processed PRI_RI_2000249.\n",
      "Processed PRI_RI_2000250.\n",
      "Processed PRI_RI_2000251.\n",
      "Processed PRI_RI_2000252.\n",
      "Processed PRI_RI_2000253.\n",
      "Processed PRI_RI_2000254.\n",
      "Processed PRI_RI_2000255.\n",
      "Processed PRI_RI_2000256.\n",
      "Processed PRI_RI_2000257.\n",
      "Processed PRI_RI_2000258.\n",
      "Processed PRI_RI_2000259.\n",
      "Processed PRI_RI_2000260.\n",
      "Processed PRI_RI_2000261.\n",
      "Processed PRI_RI_2000262.\n",
      "Processed PRI_RI_2000263.\n",
      "Processed PRI_RI_2000264.\n",
      "Processed PRI_RI_2000265.\n",
      "Processed PRI_RI_2000266.\n",
      "Processed PRI_RI_2000267.\n",
      "Processed PRI_RI_2000268.\n",
      "Processed PRI_RI_2000269.\n",
      "Processed PRI_RI_2000270.\n",
      "Processed PRI_RI_2000271.\n",
      "Processed PRI_RI_2000272.\n",
      "Processed PRI_RI_2000273.\n",
      "Processed PRI_RI_2000274.\n",
      "Processed PRI_RI_2000275.\n",
      "Processed PRI_RI_2000276.\n",
      "Processed PRI_RI_2000277.\n",
      "Processed PRI_RI_2000278.\n",
      "Processed PRI_RI_2000279.\n",
      "Processed PRI_RI_2000280.\n",
      "Processed PRI_RI_2000281.\n",
      "Processed PRI_RI_2000282.\n",
      "Processed PRI_RI_2000283.\n",
      "Processed PRI_RI_2000284.\n",
      "Processed PRI_RI_2000285.\n",
      "Processed PRI_RI_2000286.\n",
      "Processed PRI_RI_2000287.\n",
      "Processed PRI_RI_2000288.\n",
      "Processed PRI_RI_2000289.\n",
      "Processed PRI_RI_2000290.\n",
      "Processed PRI_RI_2000291.\n",
      "Processed PRI_RI_2000292.\n",
      "Processed PRI_RI_2000293.\n",
      "Processed PRI_RI_2000294.\n",
      "Processed PRI_RI_2000295.\n",
      "Processed PRI_RI_2000296.\n",
      "Processed PRI_RI_2000297.\n",
      "Processed PRI_RI_2000298.\n",
      "Processed PRI_RI_2000299.\n",
      "Processed PRI_RI_2000300.\n",
      "Processed PRI_RI_2000301.\n",
      "Processed PRI_RI_2000302.\n",
      "Processed PRI_RI_2000303.\n",
      "Processed PRI_RI_2000304.\n",
      "Processed PRI_RI_2000305.\n",
      "Processed PRI_RI_2000306.\n",
      "Processed PRI_RI_2000307.\n",
      "Processed PRI_RI_2000308.\n",
      "Processed PRI_RI_2000309.\n",
      "Processed PRI_RI_2000310.\n",
      "Processed PRI_RI_2000311.\n",
      "Processed PRI_RI_2000312.\n",
      "Processed PRI_RI_2000313.\n",
      "Processed PRI_RI_2000314.\n",
      "Processed PRI_RI_2000315.\n",
      "Processed PRI_RI_2000316.\n",
      "Processed PRI_RI_2000317.\n",
      "Processed PRI_RI_2000318.\n",
      "Processed PRI_RI_2000319.\n",
      "Processed PRI_RI_2000320.\n",
      "Processed PRI_RI_2000321.\n",
      "Processed PRI_RI_2000322.\n",
      "Processed PRI_RI_2000323.\n",
      "Processed PRI_RI_2000324.\n",
      "Processed PRI_RI_2000325.\n",
      "Processed PRI_RI_2000326.\n",
      "Processed PRI_RI_2000327.\n",
      "Processed PRI_RI_2000328.\n",
      "Processed PRI_RI_2000329.\n",
      "Processed PRI_RI_2000330.\n",
      "Processed PRI_RI_2000331.\n",
      "Processed PRI_RI_2000332.\n",
      "Processed PRI_RI_2000333.\n",
      "Processed PRI_RI_2000334.\n",
      "Processed PRI_RI_2000335.\n",
      "Processed PRI_RI_2000336.\n",
      "Processed PRI_RI_2000337.\n",
      "Processed PRI_RI_2000338.\n",
      "Processed PRI_RI_2000339.\n",
      "Processed PRI_RI_2000340.\n",
      "Processed PRI_RI_2000341.\n",
      "Processed PRI_RI_2000342.\n",
      "Processed PRI_RI_2000343.\n",
      "Processed PRI_RI_2000344.\n",
      "Processed PRI_RI_2000345.\n",
      "Processed PRI_RI_2000346.\n",
      "Processed PRI_RI_2000347.\n",
      "Processed PRI_RI_2000348.\n",
      "Processed PRI_RI_2000349.\n",
      "Processed PRI_RI_2000350.\n",
      "Processed PRI_RI_2000351.\n",
      "Processed PRI_RI_2000352.\n",
      "Processed PRI_RI_2000353.\n",
      "Processed PRI_RI_2000354.\n",
      "Processed PRI_RI_2000355.\n",
      "Processed PRI_RI_2000356.\n",
      "Processed PRI_RI_2000357.\n",
      "Processed PRI_RI_2000358.\n",
      "Processed PRI_RI_2000359.\n",
      "Processed PRI_RO_2000360.\n",
      "Processed PRI_RO_2000361.\n",
      "Processed PRI_RO_2000362.\n",
      "Processed PRI_RO_2000363.\n",
      "Processed PRI_RO_2000364.\n",
      "Processed PRI_RO_2000365.\n",
      "Processed PRI_RO_2000366.\n",
      "Processed PRI_RO_2000367.\n",
      "Processed PRI_RO_2000368.\n",
      "Processed PRI_RO_2000369.\n",
      "Processed PRI_RO_2000370.\n",
      "Processed PRI_RO_2000371.\n",
      "Processed PRI_RO_2000372.\n",
      "Processed PRI_RO_2000373.\n",
      "Processed PRI_RO_2000374.\n",
      "Processed PRI_RO_2000375.\n",
      "Processed PRI_RO_2000376.\n",
      "Processed PRI_RO_2000377.\n",
      "Processed PRI_RO_2000378.\n",
      "Processed PRI_RO_2000379.\n",
      "Processed PRI_RO_2000380.\n",
      "Processed PRI_RO_2000381.\n",
      "Processed PRI_RO_2000382.\n",
      "Processed PRI_RO_2000383.\n",
      "Processed PRI_RO_2000384.\n",
      "Processed PRI_RO_2000385.\n",
      "Processed PRI_RO_2000386.\n",
      "Processed PRI_RO_2000387.\n",
      "Processed PRI_RO_2000388.\n",
      "Processed PRI_RO_2000389.\n",
      "Processed PRI_RO_2000390.\n",
      "Processed PRI_RO_2000391.\n",
      "Processed PRI_RO_2000392.\n",
      "Processed PRI_RO_2000393.\n",
      "Processed PRI_RO_2000394.\n",
      "Processed PRI_RO_2000395.\n",
      "Processed PRI_RO_2000396.\n",
      "Processed PRI_RO_2000397.\n",
      "Processed PRI_RO_2000398.\n",
      "Processed PRI_RO_2000399.\n",
      "Processed PRI_RO_2000400.\n",
      "Processed PRI_RO_2000401.\n",
      "Processed PRI_RO_2000402.\n",
      "Processed PRI_RO_2000403.\n",
      "Processed PRI_RO_2000404.\n",
      "Processed PRI_RO_2000405.\n",
      "Processed PRI_RO_2000406.\n",
      "Processed PRI_RO_2000407.\n",
      "Processed PRI_RO_2000408.\n",
      "Processed PRI_RO_2000409.\n",
      "Processed PRI_RO_2000410.\n",
      "Processed PRI_RO_2000411.\n",
      "Processed PRI_RO_2000412.\n",
      "Processed PRI_RO_2000413.\n",
      "Processed PRI_RO_2000414.\n",
      "Processed PRI_RO_2000415.\n",
      "Processed PRI_RO_2000416.\n",
      "Processed PRI_RO_2000417.\n",
      "Processed PRI_RO_2000418.\n",
      "Processed PRI_RO_2000419.\n",
      "Processed PRI_RO_2000420.\n",
      "Processed PRI_RO_2000421.\n",
      "Processed PRI_RO_2000422.\n",
      "Processed PRI_RO_2000423.\n",
      "Processed PRI_RO_2000424.\n",
      "Processed PRI_RO_2000425.\n",
      "Processed PRI_RO_2000426.\n",
      "Processed PRI_RO_2000427.\n",
      "Processed PRI_RO_2000428.\n",
      "Processed PRI_RO_2000429.\n",
      "Processed PRI_RO_2000430.\n",
      "Processed PRI_RO_2000431.\n",
      "Processed PRI_RO_2000432.\n",
      "Processed PRI_RO_2000433.\n",
      "Processed PRI_RO_2000434.\n",
      "Processed PRI_RO_2000435.\n",
      "Processed PRI_RO_2000436.\n",
      "Processed PRI_RO_2000437.\n",
      "Processed PRI_RO_2000438.\n",
      "Processed PRI_RO_2000439.\n",
      "Processed PRI_RO_2000440.\n",
      "Processed PRI_RO_2000441.\n",
      "Processed PRI_RO_2000442.\n",
      "Processed PRI_RO_2000443.\n",
      "Processed PRI_RO_2000444.\n",
      "Processed PRI_RO_2000445.\n",
      "Processed PRI_RO_2000446.\n",
      "Processed PRI_RO_2000447.\n",
      "Processed PRI_RO_2000448.\n",
      "Processed PRI_RO_2000449.\n",
      "Processed PRI_RO_2000450.\n",
      "Processed PRI_RO_2000451.\n",
      "Processed PRI_RO_2000452.\n",
      "Processed PRI_RO_2000453.\n",
      "Processed PRI_RO_2000454.\n",
      "Processed PRI_RO_2000455.\n",
      "Processed PRI_RO_2000456.\n",
      "Processed PRI_RO_2000457.\n",
      "Processed PRI_RO_2000458.\n",
      "Processed PRI_RO_2000459.\n",
      "Processed PRI_RO_2000460.\n",
      "Processed PRI_RO_2000461.\n",
      "Processed PRI_RO_2000462.\n",
      "Processed PRI_RO_2000463.\n",
      "Processed PRI_RO_2000464.\n",
      "Processed PRI_RO_2000465.\n",
      "Processed PRI_RO_2000466.\n",
      "Processed PRI_RO_2000467.\n",
      "Processed PRI_RO_2000468.\n",
      "Processed PRI_RO_2000469.\n",
      "Processed PRI_RO_2000470.\n",
      "Processed PRI_RO_2000471.\n",
      "Processed PRI_RO_2000472.\n",
      "Processed PRI_RO_2000473.\n",
      "Processed PRI_RO_2000474.\n",
      "Processed PRI_RO_2000475.\n",
      "Processed PRI_RO_2000476.\n",
      "Processed PRI_RO_2000477.\n",
      "Processed PRI_RO_2000478.\n",
      "Processed PRI_RO_2000479.\n",
      "Processed PRI_RO_2000480.\n",
      "Processed PRI_RO_2000481.\n",
      "Processed PRI_RO_2000482.\n",
      "Processed PRI_RO_2000483.\n",
      "Processed PRI_RO_2000484.\n",
      "Processed PRI_RO_2000485.\n",
      "Processed PRI_RO_2000486.\n",
      "Processed PRI_RO_2000487.\n",
      "Processed PRI_RO_2000488.\n",
      "Processed PRI_RO_2000489.\n",
      "Processed PRI_RO_2000490.\n",
      "Processed PRI_RO_2000491.\n",
      "Processed PRI_RO_2000492.\n",
      "Processed PRI_RO_2000493.\n",
      "Processed PRI_RO_2000494.\n",
      "Processed PRI_RO_2000495.\n",
      "Processed PRI_RO_2000496.\n",
      "Processed PRI_RO_2000497.\n",
      "Processed PRI_RO_2000498.\n",
      "Processed PRI_RO_2000499.\n",
      "Processed PRI_RO_2000500.\n",
      "Processed PRI_RO_2000501.\n",
      "Processed PRI_RO_2000502.\n",
      "Processed PRI_RO_2000503.\n",
      "Processed PRI_RO_2000504.\n",
      "Processed PRI_RO_2000505.\n",
      "Processed PRI_RO_2000506.\n",
      "Processed PRI_RO_2000507.\n",
      "Processed PRI_RO_2000508.\n",
      "Processed PRI_RO_2000509.\n",
      "Processed PRI_RO_2000510.\n",
      "Processed PRI_RO_2000511.\n",
      "Processed PRI_RO_2000512.\n",
      "Processed PRI_RO_2000513.\n",
      "Processed PRI_RO_2000514.\n",
      "Processed PRI_RO_2000515.\n",
      "Processed PRI_RO_2000516.\n",
      "Processed PRI_RO_2000517.\n",
      "Processed PRI_RO_2000518.\n",
      "Processed PRI_RO_2000519.\n",
      "Processed PRI_RO_2000520.\n",
      "Processed PRI_RO_2000521.\n",
      "Processed PRI_RO_2000522.\n",
      "Processed PRI_RO_2000523.\n",
      "Processed PRI_RO_2000524.\n",
      "Processed PRI_RO_2000525.\n",
      "Processed PRI_RO_2000526.\n",
      "Processed PRI_RO_2000527.\n",
      "Processed PRI_RO_2000528.\n",
      "Processed PRI_RO_2000529.\n",
      "Processed PRI_RO_2000530.\n",
      "Processed PRI_RO_2000531.\n",
      "Processed PRI_RO_2000532.\n",
      "Processed PRI_RO_2000533.\n",
      "Processed PRI_RO_2000534.\n",
      "Processed PRI_RO_2000535.\n",
      "Processed PRI_RO_2000536.\n",
      "Processed PRI_RO_2000537.\n",
      "Processed PRI_RO_2000538.\n",
      "Processed PRI_RO_2000539.\n",
      "Processed PRI_RO_2000540.\n",
      "Processed PRI_RO_2000541.\n",
      "Processed PRI_RO_2000542.\n",
      "Processed PRI_RO_2000543.\n",
      "Processed PRI_RO_2000544.\n",
      "Processed PRI_RO_2000545.\n",
      "Processed PRI_RO_2000546.\n",
      "Processed PRI_RO_2000547.\n",
      "Processed PRI_RO_2000548.\n",
      "Processed PRI_RO_2000549.\n",
      "Processed PRI_RO_2000550.\n",
      "Processed PRI_RO_2000551.\n",
      "Processed PRI_RO_2000552.\n",
      "Processed PRI_RO_2000553.\n",
      "Processed PRI_RO_2000554.\n",
      "Processed PRI_RO_2000555.\n",
      "Processed PRI_RO_2000556.\n",
      "Processed PRI_RO_2000557.\n",
      "Processed PRI_RO_2000558.\n",
      "Processed PRI_RO_2000559.\n",
      "Processed PRI_RO_2000560.\n",
      "Processed PRI_RO_2000561.\n",
      "Processed PRI_RO_2000562.\n",
      "Processed PRI_RO_2000563.\n",
      "Processed PRI_RO_2000564.\n",
      "Processed PRI_RO_2000565.\n",
      "Processed PRI_RO_2000566.\n",
      "Processed PRI_RO_2000567.\n",
      "Processed PRI_RO_2000568.\n",
      "Processed PRI_RO_2000569.\n",
      "Processed PRI_RO_2000570.\n",
      "Processed PRI_RO_2000571.\n",
      "Processed PRI_RO_2000572.\n",
      "Processed PRI_RO_2000573.\n",
      "Processed PRI_RO_2000574.\n",
      "Processed PRI_RO_2000575.\n",
      "Processed PRI_RO_2000576.\n",
      "Processed PRI_RO_2000577.\n",
      "Processed PRI_RO_2000578.\n",
      "Processed PRI_RO_2000579.\n",
      "Processed PRI_RO_2000580.\n",
      "Processed PRI_RO_2000581.\n",
      "Processed PRI_RO_2000582.\n",
      "Processed PRI_RO_2000583.\n",
      "Processed PRI_RO_2000584.\n",
      "Processed PRI_RO_2000585.\n",
      "Processed PRI_RO_2000586.\n",
      "Processed PRI_RO_2000587.\n",
      "Processed PRI_RO_2000588.\n",
      "Processed PRI_RO_2000589.\n",
      "Processed PRI_RO_2000590.\n",
      "Processed PRI_RO_2000591.\n",
      "Processed PRI_RO_2000592.\n",
      "Processed PRI_RO_2000593.\n",
      "Processed PRI_RO_2000594.\n",
      "Processed PRI_RO_2000595.\n",
      "Processed PRI_RO_2000596.\n",
      "Processed PRI_RO_2000597.\n",
      "Processed PRI_RO_2000598.\n",
      "Processed PRI_RO_2000599.\n",
      "Processed PRI_RO_2000600.\n",
      "Processed PRI_RO_2000601.\n",
      "Processed PRI_RO_2000602.\n",
      "Processed PRI_RO_2000603.\n",
      "Processed PRI_RO_2000604.\n",
      "Processed PRI_RO_2000605.\n",
      "Processed PRI_RO_2000606.\n",
      "Processed PRI_RO_2000607.\n",
      "Processed PRI_RO_2000608.\n",
      "Processed PRI_RO_2000609.\n",
      "Processed PRI_RO_2000610.\n",
      "Processed PRI_RO_2000611.\n",
      "Processed PRI_RO_2000612.\n",
      "Processed PRI_RO_2000613.\n",
      "Processed PRI_RO_2000614.\n",
      "Processed PRI_RO_2000615.\n",
      "Processed PRI_RO_2000616.\n",
      "Processed PRI_RO_2000617.\n",
      "Processed PRI_RO_2000618.\n",
      "Processed PRI_RO_2000619.\n",
      "Processed PRI_RO_2000620.\n",
      "Processed PRI_RO_2000621.\n",
      "Processed PRI_RO_2000622.\n",
      "Processed PRI_RO_2000623.\n",
      "Processed PRI_RO_2000624.\n",
      "Processed PRI_RO_2000625.\n",
      "Processed PRI_RO_2000626.\n",
      "Processed PRI_RO_2000627.\n",
      "Processed PRI_RO_2000628.\n",
      "Processed PRI_RO_2000629.\n",
      "Processed PRI_RO_2000630.\n",
      "Processed PRI_RO_2000631.\n",
      "Processed PRI_RO_2000632.\n",
      "Processed PRI_RO_2000633.\n",
      "Processed PRI_RO_2000634.\n",
      "Processed PRI_RO_2000635.\n",
      "Processed PRI_RO_2000636.\n",
      "Processed PRI_RO_2000637.\n",
      "Processed PRI_RO_2000638.\n",
      "Processed PRI_RO_2000639.\n",
      "Processed PRI_RO_2000640.\n",
      "Processed PRI_RO_2000641.\n",
      "Processed PRI_RO_2000642.\n",
      "Processed PRI_RO_2000643.\n",
      "Processed PRI_RO_2000644.\n",
      "Processed PRI_RO_2000645.\n",
      "Processed PRI_RO_2000646.\n",
      "Processed PRI_RO_2000647.\n",
      "Processed PRI_RO_2000648.\n",
      "Processed PRI_RO_2000649.\n",
      "Processed PRI_RO_2000650.\n",
      "Processed PRI_RO_2000651.\n",
      "Processed PRI_RO_2000652.\n",
      "Processed PRI_RO_2000653.\n",
      "Processed PRI_RO_2000654.\n",
      "Processed PRI_RO_2000655.\n",
      "Processed PRI_RO_2000656.\n",
      "Processed PRI_RO_2000657.\n",
      "Processed PRI_RO_2000658.\n",
      "Processed PRI_RO_2000659.\n",
      "Processed PRI_RO_2000660.\n",
      "Processed PRI_RO_2000661.\n",
      "Processed PRI_RO_2000662.\n",
      "Processed PRI_RO_2000663.\n",
      "Processed PRI_RO_2000664.\n",
      "Processed PRI_RO_2000665.\n",
      "Processed PRI_RO_2000666.\n",
      "Processed PRI_RO_2000667.\n",
      "Processed PRI_RO_2000668.\n",
      "Processed PRI_RO_2000669.\n",
      "Processed PRI_RO_2000670.\n",
      "Processed PRI_RO_2000671.\n",
      "Processed PRI_RO_2000672.\n",
      "Processed PRI_RO_2000673.\n",
      "Processed PRI_RO_2000674.\n",
      "Processed PRI_RO_2000675.\n",
      "Processed PRI_RO_2000676.\n",
      "Processed PRI_RO_2000677.\n",
      "Processed PRI_RO_2000678.\n",
      "Processed PRI_RO_2000679.\n",
      "Processed PRI_RO_2000680.\n",
      "Processed PRI_RO_2000681.\n",
      "Processed PRI_RO_2000682.\n",
      "Processed PRI_RO_2000683.\n",
      "Processed PRI_RO_2000684.\n",
      "Processed PRI_RO_2000685.\n",
      "Processed PRI_RO_2000686.\n",
      "Processed PRI_RO_2000687.\n",
      "Processed PRI_RO_2000688.\n",
      "Processed PRI_RO_2000689.\n",
      "Processed PRI_RO_2000690.\n",
      "Processed PRI_RO_2000691.\n",
      "Processed PRI_RO_2000692.\n",
      "Processed PRI_RO_2000693.\n",
      "Processed PRI_RO_2000694.\n",
      "Processed PRI_RO_2000695.\n",
      "Processed PRI_RO_2000696.\n",
      "Processed PRI_RO_2000697.\n",
      "Processed PRI_RO_2000698.\n",
      "Processed PRI_RO_2000699.\n",
      "Processed PRI_RO_2000700.\n",
      "Processed PRI_RO_2000701.\n",
      "Processed PRI_RO_2000702.\n",
      "Processed PRI_RO_2000703.\n",
      "Processed PRI_RO_2000704.\n",
      "Processed PRI_RO_2000705.\n",
      "Processed PRI_RO_2000706.\n",
      "Processed PRI_RO_2000707.\n",
      "Processed PRI_RO_2000708.\n",
      "Processed PRI_RO_2000709.\n",
      "Processed PRI_RO_2000710.\n",
      "Processed PRI_RO_2000711.\n",
      "Processed PRI_RO_2000712.\n",
      "Processed PRI_RO_2000713.\n",
      "Processed PRI_RO_2000714.\n",
      "Processed PRI_RO_2000715.\n",
      "Processed PRI_RO_2000716.\n",
      "Processed PRI_RO_2000717.\n",
      "Processed PRI_RO_2000718.\n",
      "Processed PRI_RO_2000719.\n",
      "Processed PUB_RI_2000000.\n",
      "Processed PUB_RI_2000001.\n",
      "Processed PUB_RI_2000002.\n",
      "Processed PUB_RI_2000003.\n",
      "Processed PUB_RI_2000004.\n",
      "Processed PUB_RI_2000005.\n",
      "Processed PUB_RI_2000006.\n",
      "Processed PUB_RI_2000007.\n",
      "Processed PUB_RI_2000008.\n",
      "Processed PUB_RI_2000009.\n",
      "Processed PUB_RI_2000010.\n",
      "Processed PUB_RI_2000011.\n",
      "Processed PUB_RI_2000012.\n",
      "Processed PUB_RI_2000013.\n",
      "Processed PUB_RI_2000014.\n",
      "Processed PUB_RI_2000015.\n",
      "Processed PUB_RI_2000016.\n",
      "Processed PUB_RI_2000017.\n",
      "Processed PUB_RI_2000018.\n",
      "Processed PUB_RI_2000019.\n",
      "Processed PUB_RI_2000020.\n",
      "Processed PUB_RI_2000021.\n",
      "Processed PUB_RI_2000022.\n",
      "Processed PUB_RI_2000023.\n",
      "Processed PUB_RI_2000024.\n",
      "Processed PUB_RI_2000025.\n",
      "Processed PUB_RI_2000026.\n",
      "Processed PUB_RI_2000027.\n",
      "Processed PUB_RI_2000028.\n",
      "Processed PUB_RI_2000029.\n",
      "Processed PUB_RI_2000030.\n",
      "Processed PUB_RI_2000031.\n",
      "Processed PUB_RI_2000032.\n",
      "Processed PUB_RI_2000033.\n",
      "Processed PUB_RI_2000034.\n",
      "Processed PUB_RI_2000035.\n",
      "Processed PUB_RI_2000036.\n",
      "Processed PUB_RI_2000037.\n",
      "Processed PUB_RI_2000038.\n",
      "Processed PUB_RI_2000039.\n",
      "Processed PUB_RI_2000040.\n",
      "Processed PUB_RI_2000041.\n",
      "Processed PUB_RI_2000042.\n",
      "Processed PUB_RI_2000043.\n",
      "Processed PUB_RI_2000044.\n",
      "Processed PUB_RI_2000045.\n",
      "Processed PUB_RI_2000046.\n",
      "Processed PUB_RI_2000047.\n",
      "Processed PUB_RI_2000048.\n",
      "Processed PUB_RI_2000049.\n",
      "Processed PUB_RI_2000050.\n",
      "Processed PUB_RI_2000051.\n",
      "Processed PUB_RI_2000052.\n",
      "Processed PUB_RI_2000053.\n",
      "Processed PUB_RI_2000054.\n",
      "Processed PUB_RI_2000055.\n",
      "Processed PUB_RI_2000056.\n",
      "Processed PUB_RI_2000057.\n",
      "Processed PUB_RI_2000058.\n",
      "Processed PUB_RI_2000059.\n",
      "Processed PUB_RI_2000060.\n",
      "Processed PUB_RI_2000061.\n",
      "Processed PUB_RI_2000062.\n",
      "Processed PUB_RI_2000063.\n",
      "Processed PUB_RI_2000064.\n",
      "Processed PUB_RI_2000065.\n",
      "Processed PUB_RI_2000066.\n",
      "Processed PUB_RI_2000067.\n",
      "Processed PUB_RI_2000068.\n",
      "Processed PUB_RI_2000069.\n",
      "Processed PUB_RI_2000070.\n",
      "Processed PUB_RI_2000071.\n",
      "Processed PUB_RI_2000072.\n",
      "Processed PUB_RI_2000073.\n",
      "Processed PUB_RI_2000074.\n",
      "Processed PUB_RI_2000075.\n",
      "Processed PUB_RI_2000076.\n",
      "Processed PUB_RI_2000077.\n",
      "Processed PUB_RI_2000078.\n",
      "Processed PUB_RI_2000079.\n",
      "Processed PUB_RI_2000080.\n",
      "Processed PUB_RI_2000081.\n",
      "Processed PUB_RI_2000082.\n",
      "Processed PUB_RI_2000083.\n",
      "Processed PUB_RI_2000084.\n",
      "Processed PUB_RI_2000085.\n",
      "Processed PUB_RI_2000086.\n",
      "Processed PUB_RI_2000087.\n",
      "Processed PUB_RI_2000088.\n",
      "Processed PUB_RI_2000089.\n",
      "Processed PUB_RI_2000090.\n",
      "Processed PUB_RI_2000091.\n",
      "Processed PUB_RI_2000092.\n",
      "Processed PUB_RI_2000093.\n",
      "Processed PUB_RI_2000094.\n",
      "Processed PUB_RI_2000095.\n",
      "Processed PUB_RI_2000096.\n",
      "Processed PUB_RI_2000097.\n",
      "Processed PUB_RI_2000098.\n",
      "Processed PUB_RI_2000099.\n",
      "Processed PUB_RI_2000100.\n",
      "Processed PUB_RI_2000101.\n",
      "Processed PUB_RI_2000102.\n",
      "Processed PUB_RI_2000103.\n",
      "Processed PUB_RI_2000104.\n",
      "Processed PUB_RI_2000105.\n",
      "Processed PUB_RI_2000106.\n",
      "Processed PUB_RI_2000107.\n",
      "Processed PUB_RI_2000108.\n",
      "Processed PUB_RI_2000109.\n",
      "Processed PUB_RI_2000110.\n",
      "Processed PUB_RI_2000111.\n",
      "Processed PUB_RI_2000112.\n",
      "Processed PUB_RI_2000113.\n",
      "Processed PUB_RI_2000114.\n",
      "Processed PUB_RI_2000115.\n",
      "Processed PUB_RI_2000116.\n",
      "Processed PUB_RI_2000117.\n",
      "Processed PUB_RI_2000118.\n",
      "Processed PUB_RI_2000119.\n",
      "Processed PUB_RI_2000120.\n",
      "Processed PUB_RI_2000121.\n",
      "Processed PUB_RI_2000122.\n",
      "Processed PUB_RI_2000123.\n",
      "Processed PUB_RI_2000124.\n",
      "Processed PUB_RI_2000125.\n",
      "Processed PUB_RI_2000126.\n",
      "Processed PUB_RI_2000127.\n",
      "Processed PUB_RI_2000128.\n",
      "Processed PUB_RI_2000129.\n",
      "Processed PUB_RI_2000130.\n",
      "Processed PUB_RI_2000131.\n",
      "Processed PUB_RI_2000132.\n",
      "Processed PUB_RI_2000133.\n",
      "Processed PUB_RI_2000134.\n",
      "Processed PUB_RI_2000135.\n",
      "Processed PUB_RI_2000136.\n",
      "Processed PUB_RI_2000137.\n",
      "Processed PUB_RI_2000138.\n",
      "Processed PUB_RI_2000139.\n",
      "Processed PUB_RI_2000140.\n",
      "Processed PUB_RI_2000141.\n",
      "Processed PUB_RI_2000142.\n",
      "Processed PUB_RI_2000143.\n",
      "Processed PUB_RI_2000144.\n",
      "Processed PUB_RI_2000145.\n",
      "Processed PUB_RI_2000146.\n",
      "Processed PUB_RI_2000147.\n",
      "Processed PUB_RI_2000148.\n",
      "Processed PUB_RI_2000149.\n",
      "Processed PUB_RI_2000150.\n",
      "Processed PUB_RI_2000151.\n",
      "Processed PUB_RI_2000152.\n",
      "Processed PUB_RI_2000153.\n",
      "Processed PUB_RI_2000154.\n",
      "Processed PUB_RI_2000155.\n",
      "Processed PUB_RI_2000156.\n",
      "Processed PUB_RI_2000157.\n",
      "Processed PUB_RI_2000158.\n",
      "Processed PUB_RI_2000159.\n",
      "Processed PUB_RI_2000160.\n",
      "Processed PUB_RI_2000161.\n",
      "Processed PUB_RI_2000162.\n",
      "Processed PUB_RI_2000163.\n",
      "Processed PUB_RI_2000164.\n",
      "Processed PUB_RI_2000165.\n",
      "Processed PUB_RI_2000166.\n",
      "Processed PUB_RI_2000167.\n",
      "Processed PUB_RI_2000168.\n",
      "Processed PUB_RI_2000169.\n",
      "Processed PUB_RI_2000170.\n",
      "Processed PUB_RI_2000171.\n",
      "Processed PUB_RI_2000172.\n",
      "Processed PUB_RI_2000173.\n",
      "Processed PUB_RI_2000174.\n",
      "Processed PUB_RI_2000175.\n",
      "Processed PUB_RI_2000176.\n",
      "Processed PUB_RI_2000177.\n",
      "Processed PUB_RI_2000178.\n",
      "Processed PUB_RI_2000179.\n",
      "Processed PUB_RI_2000180.\n",
      "Processed PUB_RI_2000181.\n",
      "Processed PUB_RI_2000182.\n",
      "Processed PUB_RI_2000183.\n",
      "Processed PUB_RI_2000184.\n",
      "Processed PUB_RI_2000185.\n",
      "Processed PUB_RI_2000186.\n",
      "Processed PUB_RI_2000187.\n",
      "Processed PUB_RI_2000188.\n",
      "Processed PUB_RI_2000189.\n",
      "Processed PUB_RI_2000190.\n",
      "Processed PUB_RI_2000191.\n",
      "Processed PUB_RI_2000192.\n",
      "Processed PUB_RI_2000193.\n",
      "Processed PUB_RI_2000194.\n",
      "Processed PUB_RI_2000195.\n",
      "Processed PUB_RI_2000196.\n",
      "Processed PUB_RI_2000197.\n",
      "Processed PUB_RI_2000198.\n",
      "Processed PUB_RI_2000199.\n",
      "Processed PUB_RI_2000200.\n",
      "Processed PUB_RI_2000201.\n",
      "Processed PUB_RI_2000202.\n",
      "Processed PUB_RI_2000203.\n",
      "Processed PUB_RI_2000204.\n",
      "Processed PUB_RI_2000205.\n",
      "Processed PUB_RI_2000206.\n",
      "Processed PUB_RI_2000207.\n",
      "Processed PUB_RI_2000208.\n",
      "Processed PUB_RI_2000209.\n",
      "Processed PUB_RI_2000210.\n",
      "Processed PUB_RI_2000211.\n",
      "Processed PUB_RI_2000212.\n",
      "Processed PUB_RI_2000213.\n",
      "Processed PUB_RI_2000214.\n",
      "Processed PUB_RI_2000215.\n",
      "Processed PUB_RI_2000216.\n",
      "Processed PUB_RI_2000217.\n",
      "Processed PUB_RI_2000218.\n",
      "Processed PUB_RI_2000219.\n",
      "Processed PUB_RI_2000220.\n",
      "Processed PUB_RI_2000221.\n",
      "Processed PUB_RI_2000222.\n",
      "Processed PUB_RI_2000223.\n",
      "Processed PUB_RI_2000224.\n",
      "Processed PUB_RI_2000225.\n",
      "Processed PUB_RI_2000226.\n",
      "Processed PUB_RI_2000227.\n",
      "Processed PUB_RI_2000228.\n",
      "Processed PUB_RI_2000229.\n",
      "Processed PUB_RI_2000230.\n",
      "Processed PUB_RI_2000231.\n",
      "Processed PUB_RI_2000232.\n",
      "Processed PUB_RI_2000233.\n",
      "Processed PUB_RI_2000234.\n",
      "Processed PUB_RI_2000235.\n",
      "Processed PUB_RI_2000236.\n",
      "Processed PUB_RI_2000237.\n",
      "Processed PUB_RI_2000238.\n",
      "Processed PUB_RI_2000239.\n",
      "Processed PUB_RI_2000240.\n",
      "Processed PUB_RI_2000241.\n",
      "Processed PUB_RI_2000242.\n",
      "Processed PUB_RI_2000243.\n",
      "Processed PUB_RI_2000244.\n",
      "Processed PUB_RI_2000245.\n",
      "Processed PUB_RI_2000246.\n",
      "Processed PUB_RI_2000247.\n",
      "Processed PUB_RI_2000248.\n",
      "Processed PUB_RI_2000249.\n",
      "Processed PUB_RI_2000250.\n",
      "Processed PUB_RI_2000251.\n",
      "Processed PUB_RI_2000252.\n",
      "Processed PUB_RI_2000253.\n",
      "Processed PUB_RI_2000254.\n",
      "Processed PUB_RI_2000255.\n",
      "Processed PUB_RI_2000256.\n",
      "Processed PUB_RI_2000257.\n",
      "Processed PUB_RI_2000258.\n",
      "Processed PUB_RI_2000259.\n",
      "Processed PUB_RI_2000260.\n",
      "Processed PUB_RI_2000261.\n",
      "Processed PUB_RI_2000262.\n",
      "Processed PUB_RI_2000263.\n",
      "Processed PUB_RI_2000264.\n",
      "Processed PUB_RI_2000265.\n",
      "Processed PUB_RI_2000266.\n",
      "Processed PUB_RI_2000267.\n",
      "Processed PUB_RI_2000268.\n",
      "Processed PUB_RI_2000269.\n",
      "Processed PUB_RI_2000270.\n",
      "Processed PUB_RI_2000271.\n",
      "Processed PUB_RI_2000272.\n",
      "Processed PUB_RI_2000273.\n",
      "Processed PUB_RI_2000274.\n",
      "Processed PUB_RI_2000275.\n",
      "Processed PUB_RI_2000276.\n",
      "Processed PUB_RI_2000277.\n",
      "Processed PUB_RI_2000278.\n",
      "Processed PUB_RI_2000279.\n",
      "Processed PUB_RI_2000280.\n",
      "Processed PUB_RI_2000281.\n",
      "Processed PUB_RI_2000282.\n",
      "Processed PUB_RI_2000283.\n",
      "Processed PUB_RI_2000284.\n",
      "Processed PUB_RI_2000285.\n",
      "Processed PUB_RI_2000286.\n",
      "Processed PUB_RI_2000287.\n",
      "Processed PUB_RI_2000288.\n",
      "Processed PUB_RI_2000289.\n",
      "Processed PUB_RI_2000290.\n",
      "Processed PUB_RI_2000291.\n",
      "Processed PUB_RI_2000292.\n",
      "Processed PUB_RI_2000293.\n",
      "Processed PUB_RI_2000294.\n",
      "Processed PUB_RI_2000295.\n",
      "Processed PUB_RI_2000296.\n",
      "Processed PUB_RI_2000297.\n",
      "Processed PUB_RI_2000298.\n",
      "Processed PUB_RI_2000299.\n",
      "Processed PUB_RI_2000300.\n",
      "Processed PUB_RI_2000301.\n",
      "Processed PUB_RI_2000302.\n",
      "Processed PUB_RI_2000303.\n",
      "Processed PUB_RI_2000304.\n",
      "Processed PUB_RI_2000305.\n",
      "Processed PUB_RI_2000306.\n",
      "Processed PUB_RI_2000307.\n",
      "Processed PUB_RI_2000308.\n",
      "Processed PUB_RI_2000309.\n",
      "Processed PUB_RI_2000310.\n",
      "Processed PUB_RI_2000311.\n",
      "Processed PUB_RI_2000312.\n",
      "Processed PUB_RI_2000313.\n",
      "Processed PUB_RI_2000314.\n",
      "Processed PUB_RI_2000315.\n",
      "Processed PUB_RI_2000316.\n",
      "Processed PUB_RI_2000317.\n",
      "Processed PUB_RI_2000318.\n",
      "Processed PUB_RI_2000319.\n",
      "Processed PUB_RI_2000320.\n",
      "Processed PUB_RI_2000321.\n",
      "Processed PUB_RI_2000322.\n",
      "Processed PUB_RI_2000323.\n",
      "Processed PUB_RI_2000324.\n",
      "Processed PUB_RI_2000325.\n",
      "Processed PUB_RI_2000326.\n",
      "Processed PUB_RI_2000327.\n",
      "Processed PUB_RI_2000328.\n",
      "Processed PUB_RI_2000329.\n",
      "Processed PUB_RI_2000330.\n",
      "Processed PUB_RI_2000331.\n",
      "Processed PUB_RI_2000332.\n",
      "Processed PUB_RI_2000333.\n",
      "Processed PUB_RI_2000334.\n",
      "Processed PUB_RI_2000335.\n",
      "Processed PUB_RI_2000336.\n",
      "Processed PUB_RI_2000337.\n",
      "Processed PUB_RI_2000338.\n",
      "Processed PUB_RI_2000339.\n",
      "Processed PUB_RI_2000340.\n",
      "Processed PUB_RI_2000341.\n",
      "Processed PUB_RI_2000342.\n",
      "Processed PUB_RI_2000343.\n",
      "Processed PUB_RI_2000344.\n",
      "Processed PUB_RI_2000345.\n",
      "Processed PUB_RI_2000346.\n",
      "Processed PUB_RI_2000347.\n",
      "Processed PUB_RI_2000348.\n",
      "Processed PUB_RI_2000349.\n",
      "Processed PUB_RI_2000350.\n",
      "Processed PUB_RI_2000351.\n",
      "Processed PUB_RI_2000352.\n",
      "Processed PUB_RI_2000353.\n",
      "Processed PUB_RI_2000354.\n",
      "Processed PUB_RI_2000355.\n",
      "Processed PUB_RI_2000356.\n",
      "Processed PUB_RI_2000357.\n",
      "Processed PUB_RI_2000358.\n",
      "Processed PUB_RI_2000359.\n",
      "Processed PUB_RO_2000360.\n",
      "Processed PUB_RO_2000361.\n",
      "Processed PUB_RO_2000362.\n",
      "Processed PUB_RO_2000363.\n",
      "Processed PUB_RO_2000364.\n",
      "Processed PUB_RO_2000365.\n",
      "Processed PUB_RO_2000366.\n",
      "Processed PUB_RO_2000367.\n",
      "Processed PUB_RO_2000368.\n",
      "Processed PUB_RO_2000369.\n",
      "Processed PUB_RO_2000370.\n",
      "Processed PUB_RO_2000371.\n",
      "Processed PUB_RO_2000372.\n",
      "Processed PUB_RO_2000373.\n",
      "Processed PUB_RO_2000374.\n",
      "Processed PUB_RO_2000375.\n",
      "Processed PUB_RO_2000376.\n",
      "Processed PUB_RO_2000377.\n",
      "Processed PUB_RO_2000378.\n",
      "Processed PUB_RO_2000379.\n",
      "Processed PUB_RO_2000380.\n",
      "Processed PUB_RO_2000381.\n",
      "Processed PUB_RO_2000382.\n",
      "Processed PUB_RO_2000383.\n",
      "Processed PUB_RO_2000384.\n",
      "Processed PUB_RO_2000385.\n",
      "Processed PUB_RO_2000386.\n",
      "Processed PUB_RO_2000387.\n",
      "Processed PUB_RO_2000388.\n",
      "Processed PUB_RO_2000389.\n",
      "Processed PUB_RO_2000390.\n",
      "Processed PUB_RO_2000391.\n",
      "Processed PUB_RO_2000392.\n",
      "Processed PUB_RO_2000393.\n",
      "Processed PUB_RO_2000394.\n",
      "Processed PUB_RO_2000395.\n",
      "Processed PUB_RO_2000396.\n",
      "Processed PUB_RO_2000397.\n",
      "Processed PUB_RO_2000398.\n",
      "Processed PUB_RO_2000399.\n",
      "Processed PUB_RO_2000400.\n",
      "Processed PUB_RO_2000401.\n",
      "Processed PUB_RO_2000402.\n",
      "Processed PUB_RO_2000403.\n",
      "Processed PUB_RO_2000404.\n",
      "Processed PUB_RO_2000405.\n",
      "Processed PUB_RO_2000406.\n",
      "Processed PUB_RO_2000407.\n",
      "Processed PUB_RO_2000408.\n",
      "Processed PUB_RO_2000409.\n",
      "Processed PUB_RO_2000410.\n",
      "Processed PUB_RO_2000411.\n",
      "Processed PUB_RO_2000412.\n",
      "Processed PUB_RO_2000413.\n",
      "Processed PUB_RO_2000414.\n",
      "Processed PUB_RO_2000415.\n",
      "Processed PUB_RO_2000416.\n",
      "Processed PUB_RO_2000417.\n",
      "Processed PUB_RO_2000418.\n",
      "Processed PUB_RO_2000419.\n",
      "Processed PUB_RO_2000420.\n",
      "Processed PUB_RO_2000421.\n",
      "Processed PUB_RO_2000422.\n",
      "Processed PUB_RO_2000423.\n",
      "Processed PUB_RO_2000424.\n",
      "Processed PUB_RO_2000425.\n",
      "Processed PUB_RO_2000426.\n",
      "Processed PUB_RO_2000427.\n",
      "Processed PUB_RO_2000428.\n",
      "Processed PUB_RO_2000429.\n",
      "Processed PUB_RO_2000430.\n",
      "Processed PUB_RO_2000431.\n",
      "Processed PUB_RO_2000432.\n",
      "Processed PUB_RO_2000433.\n",
      "Processed PUB_RO_2000434.\n",
      "Processed PUB_RO_2000435.\n",
      "Processed PUB_RO_2000436.\n",
      "Processed PUB_RO_2000437.\n",
      "Processed PUB_RO_2000438.\n",
      "Processed PUB_RO_2000439.\n",
      "Processed PUB_RO_2000440.\n",
      "Processed PUB_RO_2000441.\n",
      "Processed PUB_RO_2000442.\n",
      "Processed PUB_RO_2000443.\n",
      "Processed PUB_RO_2000444.\n",
      "Processed PUB_RO_2000445.\n",
      "Processed PUB_RO_2000446.\n",
      "Processed PUB_RO_2000447.\n",
      "Processed PUB_RO_2000448.\n",
      "Processed PUB_RO_2000449.\n",
      "Processed PUB_RO_2000450.\n",
      "Processed PUB_RO_2000451.\n",
      "Processed PUB_RO_2000452.\n",
      "Processed PUB_RO_2000453.\n",
      "Processed PUB_RO_2000454.\n",
      "Processed PUB_RO_2000455.\n",
      "Processed PUB_RO_2000456.\n",
      "Processed PUB_RO_2000457.\n",
      "Processed PUB_RO_2000458.\n",
      "Processed PUB_RO_2000459.\n",
      "Processed PUB_RO_2000460.\n",
      "Processed PUB_RO_2000461.\n",
      "Processed PUB_RO_2000462.\n",
      "Processed PUB_RO_2000463.\n",
      "Processed PUB_RO_2000464.\n",
      "Processed PUB_RO_2000465.\n",
      "Processed PUB_RO_2000466.\n",
      "Processed PUB_RO_2000467.\n",
      "Processed PUB_RO_2000468.\n",
      "Processed PUB_RO_2000469.\n",
      "Processed PUB_RO_2000470.\n",
      "Processed PUB_RO_2000471.\n",
      "Processed PUB_RO_2000472.\n",
      "Processed PUB_RO_2000473.\n",
      "Processed PUB_RO_2000474.\n",
      "Processed PUB_RO_2000475.\n",
      "Processed PUB_RO_2000476.\n",
      "Processed PUB_RO_2000477.\n",
      "Processed PUB_RO_2000478.\n",
      "Processed PUB_RO_2000479.\n",
      "Processed PUB_RO_2000480.\n",
      "Processed PUB_RO_2000481.\n",
      "Processed PUB_RO_2000482.\n",
      "Processed PUB_RO_2000483.\n",
      "Processed PUB_RO_2000484.\n",
      "Processed PUB_RO_2000485.\n",
      "Processed PUB_RO_2000486.\n",
      "Processed PUB_RO_2000487.\n",
      "Processed PUB_RO_2000488.\n",
      "Processed PUB_RO_2000489.\n",
      "Processed PUB_RO_2000490.\n",
      "Processed PUB_RO_2000491.\n",
      "Processed PUB_RO_2000492.\n",
      "Processed PUB_RO_2000493.\n",
      "Processed PUB_RO_2000494.\n",
      "Processed PUB_RO_2000495.\n",
      "Processed PUB_RO_2000496.\n",
      "Processed PUB_RO_2000497.\n",
      "Processed PUB_RO_2000498.\n",
      "Processed PUB_RO_2000499.\n",
      "Processed PUB_RO_2000500.\n",
      "Processed PUB_RO_2000501.\n",
      "Processed PUB_RO_2000502.\n",
      "Processed PUB_RO_2000503.\n",
      "Processed PUB_RO_2000504.\n",
      "Processed PUB_RO_2000505.\n",
      "Processed PUB_RO_2000506.\n",
      "Processed PUB_RO_2000507.\n",
      "Processed PUB_RO_2000508.\n",
      "Processed PUB_RO_2000509.\n",
      "Processed PUB_RO_2000510.\n",
      "Processed PUB_RO_2000511.\n",
      "Processed PUB_RO_2000512.\n",
      "Processed PUB_RO_2000513.\n",
      "Processed PUB_RO_2000514.\n",
      "Processed PUB_RO_2000515.\n",
      "Processed PUB_RO_2000516.\n",
      "Processed PUB_RO_2000517.\n",
      "Processed PUB_RO_2000518.\n",
      "Processed PUB_RO_2000519.\n",
      "Processed PUB_RO_2000520.\n",
      "Processed PUB_RO_2000521.\n",
      "Processed PUB_RO_2000522.\n",
      "Processed PUB_RO_2000523.\n",
      "Processed PUB_RO_2000524.\n",
      "Processed PUB_RO_2000525.\n",
      "Processed PUB_RO_2000526.\n",
      "Processed PUB_RO_2000527.\n",
      "Processed PUB_RO_2000528.\n",
      "Processed PUB_RO_2000529.\n",
      "Processed PUB_RO_2000530.\n",
      "Processed PUB_RO_2000531.\n",
      "Processed PUB_RO_2000532.\n",
      "Processed PUB_RO_2000533.\n",
      "Processed PUB_RO_2000534.\n",
      "Processed PUB_RO_2000535.\n",
      "Processed PUB_RO_2000536.\n",
      "Processed PUB_RO_2000537.\n",
      "Processed PUB_RO_2000538.\n",
      "Processed PUB_RO_2000539.\n",
      "Processed PUB_RO_2000540.\n",
      "Processed PUB_RO_2000541.\n",
      "Processed PUB_RO_2000542.\n",
      "Processed PUB_RO_2000543.\n",
      "Processed PUB_RO_2000544.\n",
      "Processed PUB_RO_2000545.\n",
      "Processed PUB_RO_2000546.\n",
      "Processed PUB_RO_2000547.\n",
      "Processed PUB_RO_2000548.\n",
      "Processed PUB_RO_2000549.\n",
      "Processed PUB_RO_2000550.\n",
      "Processed PUB_RO_2000551.\n",
      "Processed PUB_RO_2000552.\n",
      "Processed PUB_RO_2000553.\n",
      "Processed PUB_RO_2000554.\n",
      "Processed PUB_RO_2000555.\n",
      "Processed PUB_RO_2000556.\n",
      "Processed PUB_RO_2000557.\n",
      "Processed PUB_RO_2000558.\n",
      "Processed PUB_RO_2000559.\n",
      "Processed PUB_RO_2000560.\n",
      "Processed PUB_RO_2000561.\n",
      "Processed PUB_RO_2000562.\n",
      "Processed PUB_RO_2000563.\n",
      "Processed PUB_RO_2000564.\n",
      "Processed PUB_RO_2000565.\n",
      "Processed PUB_RO_2000566.\n",
      "Processed PUB_RO_2000567.\n",
      "Processed PUB_RO_2000568.\n",
      "Processed PUB_RO_2000569.\n",
      "Processed PUB_RO_2000570.\n",
      "Processed PUB_RO_2000571.\n",
      "Processed PUB_RO_2000572.\n",
      "Processed PUB_RO_2000573.\n",
      "Processed PUB_RO_2000574.\n",
      "Processed PUB_RO_2000575.\n",
      "Processed PUB_RO_2000576.\n",
      "Processed PUB_RO_2000577.\n",
      "Processed PUB_RO_2000578.\n",
      "Processed PUB_RO_2000579.\n",
      "Processed PUB_RO_2000580.\n",
      "Processed PUB_RO_2000581.\n",
      "Processed PUB_RO_2000582.\n",
      "Processed PUB_RO_2000583.\n",
      "Processed PUB_RO_2000584.\n",
      "Processed PUB_RO_2000585.\n",
      "Processed PUB_RO_2000586.\n",
      "Processed PUB_RO_2000587.\n",
      "Processed PUB_RO_2000588.\n",
      "Processed PUB_RO_2000589.\n",
      "Processed PUB_RO_2000590.\n",
      "Processed PUB_RO_2000591.\n",
      "Processed PUB_RO_2000592.\n",
      "Processed PUB_RO_2000593.\n",
      "Processed PUB_RO_2000594.\n",
      "Processed PUB_RO_2000595.\n",
      "Processed PUB_RO_2000596.\n",
      "Processed PUB_RO_2000597.\n",
      "Processed PUB_RO_2000598.\n",
      "Processed PUB_RO_2000599.\n",
      "Processed PUB_RO_2000600.\n",
      "Processed PUB_RO_2000601.\n",
      "Processed PUB_RO_2000602.\n",
      "Processed PUB_RO_2000603.\n",
      "Processed PUB_RO_2000604.\n",
      "Processed PUB_RO_2000605.\n",
      "Processed PUB_RO_2000606.\n",
      "Processed PUB_RO_2000607.\n",
      "Processed PUB_RO_2000608.\n",
      "Processed PUB_RO_2000609.\n",
      "Processed PUB_RO_2000610.\n",
      "Processed PUB_RO_2000611.\n",
      "Processed PUB_RO_2000612.\n",
      "Processed PUB_RO_2000613.\n",
      "Processed PUB_RO_2000614.\n",
      "Processed PUB_RO_2000615.\n",
      "Processed PUB_RO_2000616.\n",
      "Processed PUB_RO_2000617.\n",
      "Processed PUB_RO_2000618.\n",
      "Processed PUB_RO_2000619.\n",
      "Processed PUB_RO_2000620.\n",
      "Processed PUB_RO_2000621.\n",
      "Processed PUB_RO_2000622.\n",
      "Processed PUB_RO_2000623.\n",
      "Processed PUB_RO_2000624.\n",
      "Processed PUB_RO_2000625.\n",
      "Processed PUB_RO_2000626.\n",
      "Processed PUB_RO_2000627.\n",
      "Processed PUB_RO_2000628.\n",
      "Processed PUB_RO_2000629.\n",
      "Processed PUB_RO_2000630.\n",
      "Processed PUB_RO_2000631.\n",
      "Processed PUB_RO_2000632.\n",
      "Processed PUB_RO_2000633.\n",
      "Processed PUB_RO_2000634.\n",
      "Processed PUB_RO_2000635.\n",
      "Processed PUB_RO_2000636.\n",
      "Processed PUB_RO_2000637.\n",
      "Processed PUB_RO_2000638.\n",
      "Processed PUB_RO_2000639.\n",
      "Processed PUB_RO_2000640.\n",
      "Processed PUB_RO_2000641.\n",
      "Processed PUB_RO_2000642.\n",
      "Processed PUB_RO_2000643.\n",
      "Processed PUB_RO_2000644.\n",
      "Processed PUB_RO_2000645.\n",
      "Processed PUB_RO_2000646.\n",
      "Processed PUB_RO_2000647.\n",
      "Processed PUB_RO_2000648.\n",
      "Processed PUB_RO_2000649.\n",
      "Processed PUB_RO_2000650.\n",
      "Processed PUB_RO_2000651.\n",
      "Processed PUB_RO_2000652.\n",
      "Processed PUB_RO_2000653.\n",
      "Processed PUB_RO_2000654.\n",
      "Processed PUB_RO_2000655.\n",
      "Processed PUB_RO_2000656.\n",
      "Processed PUB_RO_2000657.\n",
      "Processed PUB_RO_2000658.\n",
      "Processed PUB_RO_2000659.\n",
      "Processed PUB_RO_2000660.\n",
      "Processed PUB_RO_2000661.\n",
      "Processed PUB_RO_2000662.\n",
      "Processed PUB_RO_2000663.\n",
      "Processed PUB_RO_2000664.\n",
      "Processed PUB_RO_2000665.\n",
      "Processed PUB_RO_2000666.\n",
      "Processed PUB_RO_2000667.\n",
      "Processed PUB_RO_2000668.\n",
      "Processed PUB_RO_2000669.\n",
      "Processed PUB_RO_2000670.\n",
      "Processed PUB_RO_2000671.\n",
      "Processed PUB_RO_2000672.\n",
      "Processed PUB_RO_2000673.\n",
      "Processed PUB_RO_2000674.\n",
      "Processed PUB_RO_2000675.\n",
      "Processed PUB_RO_2000676.\n",
      "Processed PUB_RO_2000677.\n",
      "Processed PUB_RO_2000678.\n",
      "Processed PUB_RO_2000679.\n",
      "Processed PUB_RO_2000680.\n",
      "Processed PUB_RO_2000681.\n",
      "Processed PUB_RO_2000682.\n",
      "Processed PUB_RO_2000683.\n",
      "Processed PUB_RO_2000684.\n",
      "Processed PUB_RO_2000685.\n",
      "Processed PUB_RO_2000686.\n",
      "Processed PUB_RO_2000687.\n",
      "Processed PUB_RO_2000688.\n",
      "Processed PUB_RO_2000689.\n",
      "Processed PUB_RO_2000690.\n",
      "Processed PUB_RO_2000691.\n",
      "Processed PUB_RO_2000692.\n",
      "Processed PUB_RO_2000693.\n",
      "Processed PUB_RO_2000694.\n",
      "Processed PUB_RO_2000695.\n",
      "Processed PUB_RO_2000696.\n",
      "Processed PUB_RO_2000697.\n",
      "Processed PUB_RO_2000698.\n",
      "Processed PUB_RO_2000699.\n",
      "Processed PUB_RO_2000700.\n",
      "Processed PUB_RO_2000701.\n",
      "Processed PUB_RO_2000702.\n",
      "Processed PUB_RO_2000703.\n",
      "Processed PUB_RO_2000704.\n",
      "Processed PUB_RO_2000705.\n",
      "Processed PUB_RO_2000706.\n",
      "Processed PUB_RO_2000707.\n",
      "Processed PUB_RO_2000708.\n",
      "Processed PUB_RO_2000709.\n",
      "Processed PUB_RO_2000710.\n",
      "Processed PUB_RO_2000711.\n",
      "Processed PUB_RO_2000712.\n",
      "Processed PUB_RO_2000713.\n",
      "Processed PUB_RO_2000714.\n",
      "Processed PUB_RO_2000715.\n",
      "Processed PUB_RO_2000716.\n",
      "Processed PUB_RO_2000717.\n",
      "Processed PUB_RO_2000718.\n",
      "Processed PUB_RO_2000719.\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# Path to test data\n",
    "img_dir = './test_dataset'\n",
    "out_dir = f'./test_results_{model_path.split(\".pth\")[0]}'\n",
    "\n",
    "# Check if the directory exists\n",
    "if not os.path.exists(out_dir):\n",
    "    # If the directory does not exist, create it\n",
    "    os.makedirs(out_dir)\n",
    "    print(f\"Directory created at {out_dir}\")\n",
    "else:\n",
    "    print(f\"Directory already exists at {out_dir}\")\n",
    "\n",
    "# Load datasets\n",
    "img_files = sorted([os.path.join(img_dir, f) for f in os.listdir(img_dir) if f.endswith('.jpg')])\n",
    "test_dataset = NavImgDataset(\n",
    "    img_files, \n",
    "    preprocessing=get_preprocessing(preprocessing_fn),\n",
    "    augmentation=get_testing_augmentation()\n",
    ")\n",
    "\n",
    "model.eval()\n",
    "\n",
    "for i, img_path in enumerate(img_files):\n",
    "    # Fetch the image a single time\n",
    "    image = test_dataset[i]\n",
    "    \n",
    "    # Convert the image numpy array to a tensor\n",
    "    x_tensor = torch.from_numpy(image).to(DEVICE).unsqueeze(0)\n",
    "    \n",
    "    # Perform prediction with the model\n",
    "    with torch.no_grad():  # Ensure no gradients are calculated since we are only predicting\n",
    "        pr_mask = model(x_tensor)\n",
    "    \n",
    "    # Processing the predicted mask for visualization\n",
    "    pr_mask = pr_mask.squeeze().cpu().numpy()\n",
    "    \n",
    "    # Crop the mask to original dimensions\n",
    "    original_h, original_w = 240, 428\n",
    "    pad_top, pad_left = 8, 10\n",
    "    pr_mask = pr_mask[pad_top:pad_top + original_h, pad_left:pad_left + original_w]    \n",
    "\n",
    "    base_filename = os.path.splitext(os.path.basename(img_path))[0]\n",
    "    print(f\"Processed {base_filename}.\")\n",
    "\n",
    "    mask = Image.fromarray((pr_mask * 255).astype(np.uint8))\n",
    "    mask.save(os.path.join(out_dir, f\"{base_filename}.png\"))\n",
    "print(f\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "seg",
   "language": "python",
   "name": "seg"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
